{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84801155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import graphviz as gr\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from tsdisagg import disaggregate_series\n",
    "from timedisagg.td import TempDisagg\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import dython\n",
    "from dython.nominal import associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7cc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.path.abspath(os.path.dirname(\"__fil__\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c9c2cb",
   "metadata": {},
   "source": [
    "# Data Aggregation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530adf3",
   "metadata": {},
   "source": [
    "## OECD Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OECD_clean_input(dataframe):\n",
    "    dataframe = dataframe.astype(str).apply(lambda x: x.str.replace('\"', \"\"))\n",
    "    dataframe.rename(columns = lambda x: x.replace('\"', \"\"), inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "OECDCountryCodeSub = {\"DEU\":\"DE\", \"FRA\":\"FR\", \"SWE\":\"SE\", \"ITA\":\"IT\", \"GBR\":\"UK\", \"DNK\":\"DK\", \n",
    "                      \"AUT\":\"AT\", \"HUN\":\"HU\", \"BEL\":\"BE\", \"FIN\":\"FI\", \"IRL\":\"IE\", \"NLD\":\"NL\",\n",
    "                      \"NOR\":\"NO\", \"POL\":\"PL\", \"PRT\":\"PT\", \"CHE\":\"CH\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ec9a9",
   "metadata": {},
   "source": [
    "### Projection Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e4fbf",
   "metadata": {},
   "source": [
    "#### Absolute Level of Projection is given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1972ccc",
   "metadata": {},
   "source": [
    "##### Total Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ffeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_pop_df = pd.read_csv(os.path.join(cur_dir,\"ProjectedPopulation.csv\"), quoting=3)\n",
    "proj_pop_df = OECD_clean_input(proj_pop_df)\n",
    "proj_pop_df[\"FREQUENCY\"] = \"A\"\n",
    "\n",
    "proj_pop_split_df = proj_pop_df[(proj_pop_df[\"Age\"]==\"Total\") & (proj_pop_df[\"SEX\"]==\"T\") & (proj_pop_df[\"TIME\"]!=\"2022\")]\n",
    "proj_pop_split_df[\"Value\"] = proj_pop_split_df[\"Value\"].astype(float)/1000000\n",
    "\n",
    "pop_df = pd.read_csv(os.path.join(cur_dir,\"PopulationLevel.csv\"), quoting=3)\n",
    "pop_df = OECD_clean_input(pop_df)\n",
    "\n",
    "proj_pop_combine_df = pd.concat([pop_df,proj_pop_split_df]).reset_index(drop=True)\n",
    "proj_pop_combine_df[\"Country Code\"] = proj_pop_combine_df[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "proj_pop_combine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523043a",
   "metadata": {},
   "source": [
    "##### Elderly Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_eldpop_split_df = proj_pop_df[(proj_pop_df[\"Age\"]==\"Share of 65 and over - elderly\") & (proj_pop_df[\"SEX\"]==\"T\") & (proj_pop_df[\"TIME\"]!=\"2022\")]\n",
    "   \n",
    "edlpop_df = pd.read_csv(os.path.join(cur_dir,\"ElderlyPopulation.csv\"), quoting=3)\n",
    "edlpop_df = OECD_clean_input(edlpop_df)\n",
    "\n",
    "proj_eldpop_combine_df = pd.concat([edlpop_df,proj_eldpop_split_df]).reset_index(drop=True)\n",
    "proj_eldpop_combine_df[\"Country Code\"] = proj_eldpop_combine_df[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "proj_eldpop_combine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5f0e0",
   "metadata": {},
   "source": [
    "#### Relative Level (Growth Rate) of Projection is given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd47d5",
   "metadata": {},
   "source": [
    "##### GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d48f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Projection file to get years from 2023 and onwards\n",
    "\n",
    "proj_gdp_df = pd.read_csv(os.path.join(cur_dir,\"ProjectedGDPGrowthRate.csv\"), quoting=3)\n",
    "proj_gdp_df = OECD_clean_input(proj_gdp_df)\n",
    "proj_gdp_df[\"Country Code\"] = proj_gdp_df[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "proj_gdp_re_df = proj_gdp_df[proj_gdp_df[\"TIME\"]!=\"2022\"]\n",
    "\n",
    "proj_gdp_re_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d6801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Historical file to get only 2022 for to be used as base for growth projection\n",
    "\n",
    "gdp_df = pd.read_csv(os.path.join(cur_dir,\"GDP.csv\"), quoting=3)\n",
    "gdp_df = OECD_clean_input(gdp_df)\n",
    "gdp_df[\"Country Code\"] = gdp_df[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "gdp_re_df = gdp_df[gdp_df[\"TIME\"]=='2022']\n",
    "\n",
    "gdp_re_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining historical and projection file with the first value (2022) being actual figure and the remaining figures as projection\n",
    "\n",
    "proj_gdp_combine_df = pd.concat([gdp_re_df,proj_gdp_re_df]).sort_values(by=[\"Country Code\",\"TIME\"]).reset_index(drop=True)\n",
    "proj_gdp_combine_df[\"Value\"] = proj_gdp_combine_df[\"Value\"].astype(float)\n",
    "\n",
    "proj_gdp_combine_country_df =pd.DataFrame()\n",
    "\n",
    "for country in OECDCountryCodeSub.values():\n",
    "    country_single = proj_gdp_combine_df[proj_gdp_combine_df[\"Country Code\"]==country].reset_index(drop=True)\n",
    "    country_single.loc[0,\"ProjectedGDPGrowth\"] = country_single.loc[0,\"Value\"]\n",
    "    for n in range(1, len(country_single)):\n",
    "        previous = n-1\n",
    "        country_single.loc[n,\"ProjectedGDPGrowth\"] = (1+(country_single.loc[n,\"Value\"]/100))*country_single.loc[previous,\"ProjectedGDPGrowth\"]\n",
    "    proj_gdp_combine_country_df = pd.concat([proj_gdp_combine_country_df,country_single])\n",
    "        \n",
    "proj_gdp_combine_country_df = proj_gdp_combine_country_df.drop(\"Value\",axis=1).rename(columns={\"ProjectedGDPGrowth\":\"Value\"})\n",
    "\n",
    "\n",
    "#Add back the historical data before year 2022 for interpolation\n",
    "\n",
    "gdp_re_hist_df = gdp_df[gdp_df[\"TIME\"].astype(int)<2022]\n",
    "proj_gdp_combine_country_df = pd.concat([gdp_re_hist_df,proj_gdp_combine_country_df]).reset_index(drop=True)\n",
    "proj_gdp_combine_country_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f6905",
   "metadata": {},
   "source": [
    "#### Time Series Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae18f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for checking stationarity\n",
    "\n",
    "def stationarity_check(country: [], file, x_axis_name: str):\n",
    "    palette = sns.color_palette('Set2',len(country))\n",
    "    country_color_dict = dict(zip(country, palette))\n",
    "    \n",
    "    if 'Market EUR' in file.columns:\n",
    "        file[\"Country Code\"] = file[\"Country\"].map(country_zip)\n",
    "    else:\n",
    "        file = OECD_clean_input(file)\n",
    "        file[\"Country Code\"] = file[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "    file_time = file.copy()\n",
    "    file_time['Value'] = file_time['Value'].astype(float)\n",
    "    file_time['TIME'] = pd.to_datetime(file_time['TIME'], exact=False)\n",
    "    file_time.set_index(\"TIME\", inplace=True)\n",
    "    file_time = file_time[[\"Country Code\",\"Value\"]]\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    for c in country:\n",
    "        if 'Market EUR' in file.columns:\n",
    "            file_time_country = file_time[file_time['Country Code']==c].resample('MS').asfreq().dropna()\n",
    "        else:\n",
    "            file_time_country = file_time[file_time['Country Code']==c].resample('YS').asfreq().dropna()\n",
    "        adf, pvalue, usedlag_, nobs_, critical_values_, icbest_ = adfuller(file_time_country['Value'])\n",
    "        \n",
    "        if pvalue > 0.05:\n",
    "            print('{} - Non-Stationary: {:.3f}'.format(c, pvalue))\n",
    "        else:\n",
    "            print('{} - Stationary: {:.3f}'.format(c, pvalue))\n",
    "\n",
    "        sns.lineplot(x=file_time_country.index, y=\"Value\", data=file_time_country, c=country_color_dict[c])\n",
    "\n",
    "    plt.ylabel(ylabel=x_axis_name)\n",
    "    plt.xlabel(xlabel='Year')\n",
    "    legend_handles = [plt.Line2D([0,1],[1,1], label=c, color=country_color_dict[c], lw=2) for i, c in enumerate(country)]\n",
    "    plt.legend(handles=legend_handles, title='Countries',loc='center left', fontsize=8, bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return file_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c22eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Training and Prediction\n",
    "\n",
    "def time_series_proj(country:[],proj_file:[[]],title_name:str):\n",
    "\n",
    "    para_arima = np.arange(0,6,1)\n",
    "    proj_file_proj = pd.DataFrame()\n",
    "    fig,ax = plt.subplots(8,2,figsize=(16,30))\n",
    "\n",
    "    for i, c in enumerate(country):\n",
    "\n",
    "        #fix to yearly frequency\n",
    "        proj_file_country = proj_file[proj_file['Country Code']==c].drop('Country Code', axis=1)\n",
    "        \n",
    "        freq_check = len(pd.date_range(proj_file_country.index[-2], proj_file_country.index[-1], freq='MS'))\n",
    "        \n",
    "        if freq_check==2:\n",
    "            proj_file_country = proj_file_country.resample('MS').asfreq()\n",
    "        elif freq_check==4:\n",
    "            oidx = proj_file_country.index\n",
    "            nidx = pd.date_range(oidx.min(), oidx.max(), freq='Q')\n",
    "            proj_file_country = proj_file_country.reindex(oidx.union(nidx)).rename(columns={'Value':'Old Value'})\n",
    "            proj_file_country['Value'] = proj_file_country['Old Value'].shift(-1)\n",
    "            proj_file_country= proj_file_country.drop('Old Value',axis=1).dropna()\n",
    "        else:\n",
    "            proj_file_country = proj_file_country.resample('YS').asfreq()\n",
    "            \n",
    "        #check for optimal parameters\n",
    "        arima_model = auto_arima(proj_file_country, start_p=para_arima[0], d=para_arima[0], start_q=para_arima[0],\n",
    "                            max_p=para_arima[len(para_arima)-1], max_d=para_arima[len(para_arima)-1], max_q=para_arima[len(para_arima)-1],\n",
    "                            start_P=para_arima[0], D=para_arima[0], start_Q=para_arima[0],\n",
    "                            max_P=para_arima[len(para_arima)-1], max_D=para_arima[len(para_arima)-1], max_Q=para_arima[len(para_arima)-1],\n",
    "                            stepwise=False)\n",
    "\n",
    "        #split the data to train and test set\n",
    "        size = int(len(proj_file_country)*0.8)\n",
    "        X_train, X_test = proj_file_country[0:size], proj_file_country[size:len(proj_file_country)]\n",
    "\n",
    "        #train the model with data training set\n",
    "        if arima_model.get_params()['seasonal_order'][-1] > 1:\n",
    "            model = SARIMAX(X_train, \n",
    "                            order=arima_model.get_params()['order'], \n",
    "                            seasonal_order=arima_model.get_params()['seasonal_order'],\n",
    "                            initialization='approximate_diffuse')\n",
    "        else:\n",
    "            model = SARIMAX(X_train, \n",
    "                            order=arima_model.get_params()['order'],\n",
    "                            initialization='approximate_diffuse')\n",
    "\n",
    "        result = model.fit()\n",
    "        \n",
    "        \n",
    "        #number of years to predict\n",
    "        if freq_check==2:\n",
    "            year_pred = len(pd.date_range((proj_file_country.index[-1]+relativedelta(months=1)),\n",
    "                            pd.to_datetime('2024',format='%Y',exact=False),\n",
    "                            freq='MS'))-1\n",
    "                \n",
    "        elif freq_check==4:\n",
    "            year_pred = len(pd.date_range((proj_file_country.index[-1]+relativedelta(months=3)),\n",
    "                            pd.to_datetime('2024',format='%Y',exact=False),\n",
    "                            freq='QS'))-1\n",
    "\n",
    "        else:\n",
    "            year_pred = 2024-(proj_file_country.index[-1].year+1)\n",
    "        \n",
    "\n",
    "        #check the prediction of both training and testing set\n",
    "        train_pred = result.predict(0,size)\n",
    "        test_pred = result.predict(size, len(proj_file_country)+year_pred)\n",
    "        \n",
    "\n",
    "        #plot the prediction\n",
    "\n",
    "        ax[i][0].plot(train_pred, label= 'Training Set Prediction')\n",
    "        ax[i][0].plot(proj_file_country['Value'], label= 'Actual Figures')\n",
    "        ax[i][0].plot(test_pred, label= 'Testing Set Prediction')\n",
    "        ax[i][0].set_title('{} - Prediction with Splitting Dataset'.format(c))\n",
    "        ax[i][0].legend()\n",
    "\n",
    "\n",
    "        #train the model without splitting the data as seems like to more accurate for only predicting one year after\n",
    "        #approximate_diffuse to avoid a possible error called LU Decomposition, not sure if there is any side-effect\n",
    "        if arima_model.get_params()['seasonal_order'][-1] > 1:\n",
    "            model = SARIMAX(proj_file_country, \n",
    "                            order=arima_model.get_params()['order'], \n",
    "                            seasonal_order=arima_model.get_params()['seasonal_order'],\n",
    "                            initialization='approximate_diffuse')\n",
    "        else:\n",
    "            model = SARIMAX(proj_file_country, \n",
    "                            order=arima_model.get_params()['order'],\n",
    "                            initialization='approximate_diffuse')\n",
    "\n",
    "        result = model.fit()\n",
    "\n",
    "        #check the prediction of both training and testing set\n",
    "        train_pred = result.predict(0,size)\n",
    "        test_pred = result.predict(size, len(proj_file_country)+year_pred)\n",
    "\n",
    "        #plot the prediction\n",
    "\n",
    "        ax[i][1].plot(train_pred, label= 'Training Set Prediction')\n",
    "        ax[i][1].plot(proj_file_country['Value'], label= 'Actual Figures')\n",
    "        ax[i][1].plot(test_pred, label= 'Testing Set Prediction')\n",
    "        ax[i][1].set_title('{} - Prediction without Splitting Dataset'.format(c))\n",
    "        ax[i][1].legend()\n",
    "\n",
    "        #include the prediction value in the dataset\n",
    "        pred_df = pd.DataFrame(test_pred[-(year_pred+1):]).rename(columns={'predicted_mean':'Value'})\n",
    "        proj_file_country = pd.concat([proj_file_country,pred_df])\n",
    "        proj_file_country['Country Code'] = c\n",
    "\n",
    "        #concat all the individual back to one big table\n",
    "        proj_file_proj = pd.concat([proj_file_proj,proj_file_country])\n",
    "\n",
    "    plt.title(title_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return proj_file_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d588c",
   "metadata": {},
   "source": [
    "## Disaggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3764a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_disagg(country:[], agg_fil:[[]], col_name:str):\n",
    "    \n",
    "    final_proj_disagg = pd.DataFrame()\n",
    "    fig,ax = plt.subplots(2,4,figsize=(30,16))\n",
    "    \n",
    "    for i, c in enumerate(country):\n",
    "        \n",
    "        agg_fil_new = agg_fil.copy()\n",
    "        \n",
    "        if col_name in ts_proj:\n",
    "            agg_fil_new = agg_fil_new.reset_index()\n",
    "            agg_fil_new['TIME'] = pd.to_datetime(agg_fil_new['index'].astype(str)).dt.to_period('Y').dt.to_timestamp(how='end')\n",
    "            \n",
    "        else:\n",
    "            agg_fil_new['TIME'] = pd.to_datetime(agg_fil_new['TIME']+'-12-31',exact=False)\n",
    "            \n",
    "        agg_fil_new = agg_fil_new[agg_fil_new['Country Code']==c].set_index('TIME')[['Value']]\n",
    "        agg_fil_new['Value'] = agg_fil_new['Value'].astype(float)\n",
    "\n",
    "        oidx = agg_fil_new.index\n",
    "        nidx = pd.date_range(oidx.min(), oidx.max(), freq='M')\n",
    "        res = agg_fil_new.reindex(oidx.union(nidx)).interpolate('cubicspline')\n",
    "        \n",
    "        res['Country Code'] = c\n",
    "        final_proj_disagg = pd.concat([final_proj_disagg,res])\n",
    "        \n",
    "        ax.flatten()[i].plot(res['Value'], label= 'Disaggregated Level')\n",
    "        ax.flatten()[i].plot(agg_fil_new['Value'], label= 'Actual Level')\n",
    "        ax.flatten()[i].set_title('{} - {} Disaggregated Time Series'.format(c, col_name))\n",
    "        ax.flatten()[i].legend()\n",
    "        \n",
    "    plt.show()\n",
    "    display(final_proj_disagg)\n",
    "    print('-'*122)\n",
    "    \n",
    "    final_proj_disagg = final_proj_disagg.reset_index().rename(columns={'Value':col_name})\n",
    "    final_proj_disagg['Year'] = final_proj_disagg['index'].dt.year\n",
    "    final_proj_disagg['Month'] = final_proj_disagg['index'].dt.month\n",
    "        \n",
    "    return final_proj_disagg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85946b4a",
   "metadata": {},
   "source": [
    "### Read all and combine all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd70d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(cur_dir,\"OECD\")\n",
    "\n",
    "files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "PreAvoidDup = \"1\"\n",
    "Agg_OECD_df = Agg_df.copy()\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    if file_path.lower().endswith(\"csv\"):\n",
    "        try:\n",
    "            if 'actual' in file_path.lower():\n",
    "                content = pd.read_csv(file_path, quoting=3)\n",
    "                Agg_OECD_df = pd.merge(Agg_OECD_df, content.drop('index',axis=1), \n",
    "                                  on=[\"Country Code\", \"Year\", \"Month\"], how=\"left\")\n",
    "                \n",
    "            else:\n",
    "                content = pd.read_csv(file_path, quoting=3)\n",
    "                content = OECD_clean_input(content)\n",
    "                content[\"Value\"] = content[\"Value\"].astype(float)\n",
    "                content[\"Country Code\"] = content[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "                content[\"Year_Month\"] = content[\"TIME\"]\n",
    "                Agg_OECD_df = pd.merge(Agg_OECD_df, content[[\"Country Code\", \"Year_Month\", \"Value\"]], \n",
    "                                  on=[\"Country Code\", \"Year_Month\"], how=\"left\")\n",
    "\n",
    "                Agg_OECD_df.rename(columns={\"Value\":\"Value\"+PreAvoidDup}, inplace=True)\n",
    "\n",
    "                PreAvoidDup += \"1\"            \n",
    "                        \n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"Error parsing {file}: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Skipping non-csv file: {file}\")\n",
    "        \n",
    "OldColumnName = Agg_OECD_df.columns.tolist()\n",
    "NewColumnName = ['Year_Month', 'Year', 'Month', 'Quarter', 'Country', 'Country Code', 'FiscalYear', 'FiscalMonth', \n",
    "                 'FiscalYearMonth', 'Average Regulatory Index Score', 'Household Disposable Income per Capita', \n",
    "                 'Elderly Population Proportion', 'GDP (in millions)', 'Healthcare Spending Proportion',\n",
    "                 'Parmaceutical Spending Proportion', 'Population Level', 'Tertiary Education Level Proportion',\n",
    "                 'Inflation Rate', 'Interest Rate', 'Unemployment Rate']\n",
    "old_new_col_name_map = dict(zip(OldColumnName, NewColumnName))\n",
    "\n",
    "Agg_OECD_df.rename(columns=old_new_col_name_map, inplace=True)\n",
    "\n",
    "Agg_OECD_df[\"Elderly Population Proportion\"] = Agg_OECD_df.apply(lambda df: df[\"Elderly Population Proportion\"]*1000000 if df[\"Elderly Population Proportion\"]<0.001\n",
    "                                                      else df[\"Elderly Population Proportion\"], axis=1)\n",
    "\n",
    "Agg_OECD_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f6059",
   "metadata": {},
   "source": [
    "## Financial Statement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf14138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Financial data function for reading IS and BS\n",
    "\n",
    "def fin_data_read(folder_path:str, data_needed:str, first_file_toread:str, skiprows:int, usecols:[], filename:str,) -> pd.DataFrame:\n",
    "\n",
    "    folder_path = folder_path\n",
    "\n",
    "    files = [f for f in os.listdir(folder_path)]\n",
    "\n",
    "    sheet_pattern = re.compile(r\"^(?:Company_)?[A-Za-z]{{2}}{}$\".format(data_needed))\n",
    "\n",
    "    fin_df = pd.DataFrame({\"YearMonth_Country_FinItems\":[\"Year_Month\",\"Country\"]})\n",
    "    sup_df = pd.read_excel(first_file_toread, \n",
    "                           sheet_name=\"Company{}\".format(data_needed), skiprows=skiprows, usecols=[0])\n",
    "    sup_df.columns = fin_df.columns\n",
    "    fin_df = pd.concat([fin_df, sup_df], ignore_index=True)\n",
    "\n",
    "    Avoid_Dup = \"1\"\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        if file.upper().startswith(filename):\n",
    "            file_ext_name = os.path.basename(file_path)\n",
    "            file_year_month = file_ext_name[-9:-5]\n",
    "\n",
    "            file_df = pd.ExcelFile(file_path)\n",
    "            all_sheet_names = file_df.sheet_names\n",
    "\n",
    "            for sheet_name in all_sheet_names:\n",
    "                if sheet_name != \"Company_BS\":\n",
    "                    if sheet_pattern.match(sheet_name):\n",
    "                        file_country = sheet_name\n",
    "                        sup_data = file_df.parse(sheet_name, skiprows=skiprows, usecols=usecols)\n",
    "                        sup_data.columns = [\"YearMonth_Country_FinItems\",\"YearMonth_Country_FinData\"+Avoid_Dup]\n",
    "\n",
    "                        month_data = pd.DataFrame({\"YearMonth_Country_FinItems\":[\"Year_Month\",\"Country\"],\n",
    "                                                  \"YearMonth_Country_FinData\"+Avoid_Dup:[file_year_month,file_country]})\n",
    "\n",
    "                        fin1_df = pd.concat([month_data,sup_data], ignore_index=True)\n",
    "                        try:\n",
    "                            fin1_df.iloc[9,0] = fin1_df.iloc[9,0].replace(\"Payments\",\"Payments_sub\")\n",
    "\n",
    "                        except:\n",
    "                            pass\n",
    "                        fin_df = pd.merge(fin_df, fin1_df, on=\"YearMonth_Country_FinItems\", how=\"outer\")\n",
    "                        fin_df = fin_df.dropna(subset=[\"YearMonth_Country_FinItems\"], how=\"all\")\n",
    "\n",
    "                        Avoid_Dup += \"1\"\n",
    "\n",
    "    return fin_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05965b10",
   "metadata": {},
   "source": [
    "## Datahub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c3e35",
   "metadata": {},
   "source": [
    "#### Price Range Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_quantiles = pd.qcut(outbound_excl_cus[\"OrderPriceSales\"], q=[0,0.2,0.4,0.6,0.8,1.0], labels=[\"Very Low\",\"Low\", \"Medium\", \"High\", \"Very High\"])\n",
    "\n",
    "outbound_price_range = outbound_excl_cus.loc[:]\n",
    "outbound_price_range[\"Price Categorization\"] = price_quantiles\n",
    "\n",
    "outbound_price_range = outbound_price_range.groupby([\"Year_Month\",\"Country Code\",\"Price Categorization\"])[\"Deliver Export Turnover\"].sum().reset_index()\n",
    "outbound_tot_price = outbound_price_range.groupby([\"Year_Month\",\"Country Code\"])[\"Deliver Export Turnover\"].transform(\"sum\")\n",
    "\n",
    "outbound_price_range[\"Price Range Revenue Proportion\"] = outbound_price_range[\"Deliver Export Turnover\"]/outbound_tot_price\n",
    "\n",
    "outbound_price_range_pivot = outbound_price_range.drop(\"Deliver Export Turnover\", axis=1).pivot_table(index=[\"Year_Month\",\"Country Code\"],\n",
    "                                                                                                     columns =[\"Price Categorization\"],\n",
    "                                                                                                     values=[\"Price Range Revenue Proportion\"],\n",
    "                                                                                                     aggfunc=\"sum\",\n",
    "                                                                                                     fill_value=0)\n",
    "\n",
    "outbound_price_range_pivot.columns = [f\"{col[1]}_{col[0]}\" for col in outbound_price_range_pivot.columns]\n",
    "outbound_price_range_pivot.reset_index(inplace=True)\n",
    "\n",
    "outbound_price_range_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4102ce",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b117dc3",
   "metadata": {},
   "source": [
    "### All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3861ee",
   "metadata": {},
   "source": [
    "#### PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3736648",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer = Pipeline(steps=[('NaNnum',SimpleImputer(strategy='mean')),\n",
    "                                  ('scale',StandardScaler())])\n",
    "\n",
    "bi_cat_transformer = Pipeline(steps=[('NaNcat',SimpleImputer(strategy='most_frequent')),\n",
    "                                  ('onehot',OneHotEncoder())])\n",
    "\n",
    "preprocessor_pca = ColumnTransformer(transformers=[('num_transformer',num_transformer,target_features+numeric_features),\n",
    "                                               ('bi_cat_transformer',bi_cat_transformer,binary_categorical_features)])\n",
    "\n",
    "pipeline_pca = Pipeline(steps=[('preprocessor',preprocessor_pca),\n",
    "                           ('pca',PCA(n_components=2))])\n",
    "\n",
    "pca = pipeline_pca.fit_transform(Agg_df_infadj[target_features+numeric_features+binary_categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Set2',len(country))\n",
    "country_color_dict = dict(zip(country,palette))\n",
    "country_color = [country_color_dict[x] for x in Agg_df_infadj[\"Country\"]]\n",
    "\n",
    "for sample in range(len(pca)):\n",
    "    plt.scatter(pca[sample][0],pca[sample][1],color=country_color[sample],s=8)\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('PCA Analysis')\n",
    "legend_handles = [plt.Line2D([0],[0], marker='o', color='w', label=label,\n",
    "                             markerfacecolor=palette[i], markersize=8) for i, label in enumerate(country)]\n",
    "plt.legend(handles=legend_handles, title='Countries',loc='center left', fontsize=8, bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173206f3",
   "metadata": {},
   "source": [
    "#### Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics for numeric features\n",
    "\n",
    "summary_stat_num = Agg_df_infadj[target_features+numeric_features].describe()\n",
    "summary_stat_num.loc[\"+3_std\"] = summary_stat_num.loc[\"mean\"]+(summary_stat_num.loc[\"std\"]*3)\n",
    "summary_stat_num.loc[\"-3_std\"] = summary_stat_num.loc[\"mean\"]-(summary_stat_num.loc[\"std\"]*3)\n",
    "\n",
    "summary_stat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aede980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Possible outliers outside of +/-3 std\n",
    "pos_out_num = Agg_df_infadj[(np.abs(stats.zscore(Agg_df_infadj[target_features+numeric_features]))>3).any(axis=1)]\n",
    "pos_out_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f5d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Correlation Matrix\n",
    "corr_num = Agg_df_infadj[target_features+numeric_features].corr()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(corr_num, xticklabels=corr_num.columns, yticklabels=corr_num.columns, cmap=\"RdBu\")\n",
    "plt.show()\n",
    "\n",
    "display(corr_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211d272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(corr_num, alpha=1, figsize=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42b6c5",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd399932",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stat_bi_cat = Agg_df_infadj[binary_categorical_features].describe(include=[\"category\",\"object\"])\n",
    "\n",
    "summary_stat_bi_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda97c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from github (dython)\n",
    "associations(Agg_df_infadj[binary_categorical_features], nom_nom_assoc=\"theil\", figsize=(15, 15), cmap='RdBu')[\"corr\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b33559",
   "metadata": {},
   "source": [
    "## Main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbee709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from github (dython)\n",
    "associations(Agg_df_infadj[target_features+main_numeric_features+main_binary_categorical_features], nom_num_assoc=\"correlation_ratio\", figsize=(15, 15), cmap='RdBu')['corr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f1a71",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4a937",
   "metadata": {},
   "source": [
    "#### Numeric main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not transformed with log\n",
    "\n",
    "fig, axes = plt.subplots(6, 4, figsize=(25,20))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(target_features+main_numeric_features):\n",
    "        sns.histplot(x=Agg_df_infadj[target_features+main_numeric_features].iloc[:,i], ax=ax)\n",
    "        ax.axvline(Agg_df_infadj[target_features+main_numeric_features].iloc[:,i].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "        ax.axvline(Agg_df_infadj[target_features+main_numeric_features].iloc[:,i].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
    "        ax.set_title(Agg_df_infadj[target_features+main_numeric_features].columns[i])\n",
    "        ax.set_xlabel(xlabel='')\n",
    "        ax.set_ylabel(ylabel='')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "fig.suptitle(\"Univariate Analysis of Key Numeric Features\", fontsize='x-large')       \n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d498c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Transformed with log\n",
    "\n",
    "fig, axes = plt.subplots(6, 4, figsize=(25,20))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(target_features+main_numeric_features):\n",
    "        sns.histplot(x=np.log1p(Agg_df_infadj[target_features+main_numeric_features]).iloc[:,i], ax=ax)\n",
    "        ax.axvline(np.log1p(Agg_df_infadj[target_features+main_numeric_features]).iloc[:,i].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "        ax.axvline(np.log1p(Agg_df_infadj[target_features+main_numeric_features]).iloc[:,i].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
    "        ax.set_title(Agg_df_infadj[target_features+main_numeric_features].columns[i])\n",
    "        ax.set_xlabel(xlabel='')\n",
    "        ax.set_ylabel(ylabel='')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "fig.suptitle(\"Univariate Analysis of Key Numeric Features (Log Transformed)\", fontsize='x-large')       \n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d71b8d",
   "metadata": {},
   "source": [
    "#### Categorical main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810815f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(10,10), gridspec_kw={'hspace':0.6})\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < 2:\n",
    "        sns.countplot(x=Agg_df_infadj[main_binary_categorical_features].iloc[:,i], ax=ax, palette='Blues')\n",
    "        ax.set_title(Agg_df_infadj[main_binary_categorical_features].columns[i])\n",
    "        ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "        ax.set_xlabel(xlabel='')\n",
    "        ax.set_ylabel(ylabel='')\n",
    "    else:\n",
    "        sns.countplot(x=Agg_df_infadj[main_binary_categorical_features].iloc[:,i], ax=ax, palette='Blues')\n",
    "        ax.set_title(Agg_df_infadj[main_binary_categorical_features].columns[i])\n",
    "        ax.set_xlabel(xlabel='')\n",
    "        ax.set_ylabel(ylabel='')\n",
    "    \n",
    "fig.suptitle(\"Univariate Analysis of Key Binary and Categorical Features\",fontsize='x-large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d42b1f",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebab501",
   "metadata": {},
   "source": [
    "#### Numeric main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42868c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(Agg_df_infadj, vars=target_features+main_numeric_features, hue='Country', corner=True, palette='Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a1bd8",
   "metadata": {},
   "source": [
    "#### Categorical main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312506d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,4,figsize=(35,15),sharey='row', sharex='col')\n",
    "\n",
    "\n",
    "for t_i, target in enumerate(target_features):\n",
    "    for p_i, pred in enumerate(main_binary_categorical_features):\n",
    "        if t_i==0 and p_i==0:\n",
    "            ax=axes[t_i][p_i]\n",
    "            sns.boxplot(Agg_df_infadj,\n",
    "                x=pred, \n",
    "                y=target, \n",
    "                hue=\"Country\", \n",
    "                palette='Set2',\n",
    "                ax=ax)\n",
    "            ax.set_xlabel(xlabel='')\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            ax.get_legend().remove()\n",
    "            \n",
    "        elif t_i==1 and p_i==0:\n",
    "            ax=axes[t_i][p_i]\n",
    "            sns.boxplot(Agg_df_infadj,\n",
    "                x=pred, \n",
    "                y=target, \n",
    "                hue=\"Country\", \n",
    "                palette='Set2',\n",
    "                ax=ax)\n",
    "            ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "            ax.get_legend().remove()\n",
    "            \n",
    "        elif t_i==1 and p_i in range(1,len(main_binary_categorical_features)):\n",
    "            ax=axes[t_i][p_i]\n",
    "            sns.boxplot(Agg_df_infadj,\n",
    "                x=pred, \n",
    "                y=target, \n",
    "                hue=\"Country\", \n",
    "                palette='Set2',\n",
    "                ax=ax)\n",
    "            if p_i == 1:\n",
    "                ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "            ax.set_ylabel(ylabel='')\n",
    "            ax.get_legend().remove()\n",
    "        \n",
    "        else:\n",
    "            ax=axes[t_i][p_i]\n",
    "            sns.boxplot(Agg_df_infadj,\n",
    "                x=pred, \n",
    "                y=target, \n",
    "                hue=\"Country\", \n",
    "                palette='Set2',\n",
    "                ax=ax)\n",
    "            ax.set_xlabel(xlabel='')\n",
    "            ax.set_ylabel(ylabel='')\n",
    "            ax.get_legend().remove()\n",
    "            \n",
    "fig.legend(handles,labels,loc='outside right center', title=\"Country\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78992616",
   "metadata": {},
   "source": [
    "## Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Train the model\n",
    "\n",
    "folds = 5\n",
    "cv = KFold(n_splits=folds-1)\n",
    "param_grid = {'rf__max_depth': [10,20,30,40,50],\n",
    "              'rf__n_estimators': np.arange(50,350,50)}\n",
    "\n",
    "preprocessor_rf = ColumnTransformer(transformers=[('num_transformer',num_transformer,numeric_features),\n",
    "                                               ('bi_cat_transformer',bi_cat_transformer,binary_categorical_features)])\n",
    "\n",
    "pipeline_rf = Pipeline(steps=[('preprocessor',preprocessor_rf),\n",
    "                           ('rf',RandomForestRegressor(n_estimators=100, random_state=0))])\n",
    "\n",
    "\n",
    "GSCV_rf = GridSearchCV(estimator=pipeline_rf, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=cv)\n",
    "\n",
    "X = Agg_df_infadj[numeric_features+binary_categorical_features]\n",
    "\n",
    "for i in target_features:\n",
    "    y = SimpleImputer(strategy='mean').fit_transform(Agg_df_infadj[[i]]).reshape(-1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/folds, random_state=0)\n",
    "\n",
    "    GSCV_rf.fit(X_train,y_train)\n",
    "\n",
    "    \n",
    "    ####Model Evaluation and Prediction\n",
    "\n",
    "    print('GridSearchCV Random Forest NMAE Best Score for Target Feature - {}: {:.3f}'.format(i,GSCV_rf.best_score_))\n",
    "    print('GridSearchCV Random Forest NMAE Best Score for Target Feature - {}: {}'.format(i,GSCV_rf.best_params_))\n",
    "    print('-'*100)\n",
    "\n",
    "    y_pred = GSCV_rf.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print('MSE - {}: {:.3f}'.format(i,mse))\n",
    "    print('RMSE - {}: {:.3f}'.format(i,rmse))\n",
    "    print('R2 - {}: {:.3f}'.format(i,r2))\n",
    "    print('-'*100)\n",
    "\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.xlabel('Actual Value')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    plt.title('Random Forest Predictions (All Features) - {}'.format(i))\n",
    "    coeff = np.polyfit(y_test,y_pred,1)\n",
    "    p_line = np.poly1d(coeff)\n",
    "    plt.plot(y_test,p_line(y_test),color='darkorange')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    ####Feature Importance (MDI - Impurity)\n",
    "\n",
    "    rf_feat_name = [name.split('__')[1] for name in GSCV_rf.best_estimator_[:-1].get_feature_names_out()]\n",
    "    rf_feat_imp = GSCV_rf.best_estimator_[-1].feature_importances_\n",
    "\n",
    "    feat_imp_df = pd.Series(rf_feat_imp, index=rf_feat_name).sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.barplot(y=feat_imp_df.index,x=feat_imp_df,palette='Blues_r')\n",
    "    plt.title('Random Forest Feature Importance (MDI) - {}'.format(i),fontsize='xx-large')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    ####Feature Importance (MDA - Accurcy - Permutation)\n",
    "    \n",
    "    #On training data\n",
    "    feat_MDA_result = permutation_importance(\n",
    "        GSCV_rf, X_train, y_train, n_repeats=10, random_state=0, n_jobs=2)\n",
    "\n",
    "    sorted_importance_idx = feat_MDA_result.importances_mean.argsort()\n",
    "\n",
    "    feat_imp_pi_df = pd.DataFrame(feat_MDA_result.importances[sorted_importance_idx].T,\n",
    "                                  columns=X.columns)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.boxplot(feat_imp_pi_df,orient='h',palette='Blues')\n",
    "    plt.axvline(x=0, color=\"darkorange\", linestyle=\"--\", linewidth=2)\n",
    "    plt.title('Random Forest Feature Importance_Training Set (MDA) - {}'.format(i),fontsize='xx-large')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #On testing data\n",
    "    feat_MDA_result = permutation_importance(\n",
    "        GSCV_rf, X_test, y_test, n_repeats=10, random_state=0, n_jobs=2)\n",
    "\n",
    "    sorted_importance_idx = feat_MDA_result.importances_mean.argsort()\n",
    "\n",
    "    feat_imp_pi_df = pd.DataFrame(feat_MDA_result.importances[sorted_importance_idx].T,\n",
    "                                  columns=X.columns)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.boxplot(feat_imp_pi_df,orient='h',palette='Blues')\n",
    "    plt.axvline(x=0, color=\"darkorange\", linestyle=\"--\", linewidth=2)\n",
    "    plt.title('Random Forest Feature Importance_Test Set (MDA) - {}'.format(i),fontsize='xx-large')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169a8a0",
   "metadata": {},
   "source": [
    "# DAG Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e150dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show different types of Basic DAGs\n",
    "\n",
    "DAG_chain = gr.Digraph()\n",
    "DAG_chain.edge(\"X\", \"Y\")\n",
    "DAG_chain.edge(\"Y\", \"Z\")\n",
    "DAG_chain\n",
    "\n",
    "DAG_fork = gr.Digraph()\n",
    "DAG_fork.edge(\"X\", \"Y\")\n",
    "DAG_fork.edge(\"X\", \"Z\")\n",
    "DAG_fork\n",
    "\n",
    "DAG_collider = gr.Digraph()\n",
    "DAG_collider.edge(\"X\", \"Z\")\n",
    "DAG_collider.edge(\"Y\", \"Z\")\n",
    "DAG_collider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the DAGs\n",
    "\n",
    "DAG_chain.render(\"Chain_DAG\",view=True,format=\"png\")\n",
    "DAG_fork.render(\"Fork_DAG\",view=True,format=\"png\")\n",
    "DAG_collider.render(\"Collider_DAG\",view=True,format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee055e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backdoor Adjustment\n",
    "\n",
    "DAG_b4Backdoor = gr.Digraph()\n",
    "DAG_b4Backdoor.edge(\"X\", \"Y\")\n",
    "DAG_b4Backdoor.edge(\"X\", \"Z\")\n",
    "DAG_b4Backdoor.edge(\"Y\", \"Z\")\n",
    "DAG_b4Backdoor\n",
    "\n",
    "DAG_afterBackdoor = gr.Digraph()\n",
    "DAG_afterBackdoor.edge(\"X\", \"Z\")\n",
    "DAG_afterBackdoor.edge(\"Y\", \"Z\")\n",
    "DAG_afterBackdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22728b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the DAGs\n",
    "\n",
    "DAG_b4Backdoor.render(\"b4Backdoor_DAG\",view=True,format=\"png\")\n",
    "DAG_afterBackdoor.render(\"afterBackdoor_DAG\",view=True,format=\"png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
