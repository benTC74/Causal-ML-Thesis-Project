{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84801155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import *\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import perf_counter\n",
    "\n",
    "import pydot\n",
    "import graphviz as gr\n",
    "from networkx.drawing.nx_pydot import to_pydot\n",
    "import networkx as nx\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from dowhy import CausalModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pmdarima.arima import auto_arima\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LassoCV, LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.base import clone\n",
    "\n",
    "import dython\n",
    "from dython.nominal import associations\n",
    "\n",
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "\n",
    "from econml.dml import CausalForestDML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7cc9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cf612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.path.abspath(os.path.dirname(\"__fil__\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c9c2cb",
   "metadata": {},
   "source": [
    "# Data Aggregation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530adf3",
   "metadata": {},
   "source": [
    "## OECD Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OECD_clean_input(dataframe):\n",
    "    dataframe = dataframe.astype(str).apply(lambda x: x.str.replace('\"', \"\"))\n",
    "    dataframe.rename(columns = lambda x: x.replace('\"', \"\"), inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "OECDCountryCodeSub = {\"DEU\":\"DE\", \"FRA\":\"FR\", \"SWE\":\"SE\", \"ITA\":\"IT\", \"GBR\":\"UK\", \"DNK\":\"DK\", \n",
    "                      \"AUT\":\"AT\", \"HUN\":\"HU\", \"BEL\":\"BE\", \"FIN\":\"FI\", \"IRL\":\"IE\", \"NLD\":\"NL\",\n",
    "                      \"NOR\":\"NO\", \"POL\":\"PL\", \"PRT\":\"PT\", \"CHE\":\"CH\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ec9a9",
   "metadata": {},
   "source": [
    "### Projection Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e4fbf",
   "metadata": {},
   "source": [
    "#### Absolute Level of Projection is given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1972ccc",
   "metadata": {},
   "source": [
    "##### Total Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ffeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_pop_df = pd.read_csv(os.path.join(cur_dir,\"ProjectedPopulation.csv\"), quoting=3)\n",
    "proj_pop_df = OECD_clean_input(proj_pop_df)\n",
    "proj_pop_df[\"FREQUENCY\"] = \"A\"\n",
    "\n",
    "proj_pop_split_df = proj_pop_df[(proj_pop_df[\"Age\"]==\"Total\") & (proj_pop_df[\"SEX\"]==\"T\") & (proj_pop_df[\"TIME\"]!=\"2022\")]\n",
    "proj_pop_split_df[\"Value\"] = proj_pop_split_df[\"Value\"].astype(float)/1000000\n",
    "\n",
    "pop_df = pd.read_csv(os.path.join(cur_dir,\"PopulationLevel.csv\"), quoting=3)\n",
    "pop_df = OECD_clean_input(pop_df)\n",
    "\n",
    "proj_pop_combine_df = pd.concat([pop_df,proj_pop_split_df]).reset_index(drop=True)\n",
    "proj_pop_combine_df[\"Country Code\"] = proj_pop_combine_df[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "proj_pop_combine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523043a",
   "metadata": {},
   "source": [
    "##### Elderly Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_eldpop_split_df = proj_pop_df[(proj_pop_df[\"Age\"]==\"Share of 65 and over - elderly\") & (proj_pop_df[\"SEX\"]==\"T\") & (proj_pop_df[\"TIME\"]!=\"2022\")]\n",
    "   \n",
    "edlpop_df = pd.read_csv(os.path.join(cur_dir,\"ElderlyPopulation.csv\"), quoting=3)\n",
    "edlpop_df = OECD_clean_input(edlpop_df)\n",
    "\n",
    "proj_eldpop_combine_df = pd.concat([edlpop_df,proj_eldpop_split_df]).reset_index(drop=True)\n",
    "proj_eldpop_combine_df[\"Country Code\"] = proj_eldpop_combine_df[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "proj_eldpop_combine_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5f0e0",
   "metadata": {},
   "source": [
    "#### Relative Level (Growth Rate) of Projection is given"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd47d5",
   "metadata": {},
   "source": [
    "##### GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d48f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Projection file to get years from 2023 and onwards\n",
    "\n",
    "proj_gdp_df = pd.read_csv(os.path.join(cur_dir,\"ProjectedGDPGrowthRate.csv\"), quoting=3)\n",
    "proj_gdp_df = OECD_clean_input(proj_gdp_df)\n",
    "proj_gdp_df[\"Country Code\"] = proj_gdp_df[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "proj_gdp_re_df = proj_gdp_df[proj_gdp_df[\"TIME\"]!=\"2022\"]\n",
    "\n",
    "proj_gdp_re_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d6801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Historical file to get only 2022 for to be used as base for growth projection\n",
    "\n",
    "gdp_df = pd.read_csv(os.path.join(cur_dir,\"GDP.csv\"), quoting=3)\n",
    "gdp_df = OECD_clean_input(gdp_df)\n",
    "gdp_df[\"Country Code\"] = gdp_df[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "gdp_re_df = gdp_df[gdp_df[\"TIME\"]=='2022']\n",
    "\n",
    "gdp_re_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining historical and projection file with the first value (2022) being actual figure and the remaining figures as projection\n",
    "\n",
    "proj_gdp_combine_df = pd.concat([gdp_re_df,proj_gdp_re_df]).sort_values(by=[\"Country Code\",\"TIME\"]).reset_index(drop=True)\n",
    "proj_gdp_combine_df[\"Value\"] = proj_gdp_combine_df[\"Value\"].astype(float)\n",
    "\n",
    "proj_gdp_combine_country_df =pd.DataFrame()\n",
    "\n",
    "for country in OECDCountryCodeSub.values():\n",
    "    country_single = proj_gdp_combine_df[proj_gdp_combine_df[\"Country Code\"]==country].reset_index(drop=True)\n",
    "    country_single.loc[0,\"ProjectedGDPGrowth\"] = country_single.loc[0,\"Value\"]\n",
    "    for n in range(1, len(country_single)):\n",
    "        previous = n-1\n",
    "        country_single.loc[n,\"ProjectedGDPGrowth\"] = (1+(country_single.loc[n,\"Value\"]/100))*country_single.loc[previous,\"ProjectedGDPGrowth\"]\n",
    "    proj_gdp_combine_country_df = pd.concat([proj_gdp_combine_country_df,country_single])\n",
    "        \n",
    "proj_gdp_combine_country_df = proj_gdp_combine_country_df.drop(\"Value\",axis=1).rename(columns={\"ProjectedGDPGrowth\":\"Value\"})\n",
    "\n",
    "\n",
    "#Add back the historical data before year 2022 for interpolation\n",
    "\n",
    "gdp_re_hist_df = gdp_df[gdp_df[\"TIME\"].astype(int)<2022]\n",
    "proj_gdp_combine_country_df = pd.concat([gdp_re_hist_df,proj_gdp_combine_country_df]).reset_index(drop=True)\n",
    "proj_gdp_combine_country_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f6905",
   "metadata": {},
   "source": [
    "#### Time Series Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae18f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for checking stationarity\n",
    "\n",
    "def stationarity_check(country: [], file, x_axis_name: str):\n",
    "    palette = sns.color_palette('Set2',len(country))\n",
    "    country_color_dict = dict(zip(country, palette))\n",
    "    \n",
    "    if 'Market EUR' in file.columns:\n",
    "        file[\"Country Code\"] = file[\"Country\"].map(country_zip)\n",
    "    else:\n",
    "        file = OECD_clean_input(file)\n",
    "        file[\"Country Code\"] = file[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "\n",
    "    file_time = file.copy()\n",
    "    file_time['Value'] = file_time['Value'].astype(float)\n",
    "    file_time['TIME'] = pd.to_datetime(file_time['TIME'], exact=False)\n",
    "    file_time.set_index(\"TIME\", inplace=True)\n",
    "    file_time = file_time[[\"Country Code\",\"Value\"]]\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    for c in country:\n",
    "        if 'Market EUR' in file.columns:\n",
    "            file_time_country = file_time[file_time['Country Code']==c].resample('MS').asfreq().dropna()\n",
    "        else:\n",
    "            file_time_country = file_time[file_time['Country Code']==c].resample('YS').asfreq().dropna()\n",
    "        adf, pvalue, usedlag_, nobs_, critical_values_, icbest_ = adfuller(file_time_country['Value'])\n",
    "        \n",
    "        if pvalue > 0.05:\n",
    "            print('{} - Non-Stationary: {:.3f}'.format(c, pvalue))\n",
    "        else:\n",
    "            print('{} - Stationary: {:.3f}'.format(c, pvalue))\n",
    "\n",
    "        sns.lineplot(x=file_time_country.index, y=\"Value\", data=file_time_country, c=country_color_dict[c])\n",
    "\n",
    "    plt.ylabel(ylabel=x_axis_name)\n",
    "    plt.xlabel(xlabel='Year')\n",
    "    legend_handles = [plt.Line2D([0,1],[1,1], label=c, color=country_color_dict[c], lw=2) for i, c in enumerate(country)]\n",
    "    plt.legend(handles=legend_handles, title='Countries',loc='center left', fontsize=8, bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return file_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c22eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Series Training and Prediction\n",
    "\n",
    "def time_series_proj(country:[],proj_file:[[]],title_name:str):\n",
    "\n",
    "    para_arima = np.arange(0,6,1)\n",
    "    proj_file_proj = pd.DataFrame()\n",
    "    fig,ax = plt.subplots(8,2,figsize=(16,30))\n",
    "\n",
    "    for i, c in enumerate(country):\n",
    "\n",
    "        #fix to yearly frequency\n",
    "        proj_file_country = proj_file[proj_file['Country Code']==c].drop('Country Code', axis=1)\n",
    "        \n",
    "        freq_check = len(pd.date_range(proj_file_country.index[-2], proj_file_country.index[-1], freq='MS'))\n",
    "        \n",
    "        if freq_check==2:\n",
    "            proj_file_country = proj_file_country.resample('MS').asfreq()\n",
    "        elif freq_check==4:\n",
    "            oidx = proj_file_country.index\n",
    "            nidx = pd.date_range(oidx.min(), oidx.max(), freq='Q')\n",
    "            proj_file_country = proj_file_country.reindex(oidx.union(nidx)).rename(columns={'Value':'Old Value'})\n",
    "            proj_file_country['Value'] = proj_file_country['Old Value'].shift(-1)\n",
    "            proj_file_country= proj_file_country.drop('Old Value',axis=1).dropna()\n",
    "        else:\n",
    "            proj_file_country = proj_file_country.resample('YS').asfreq()\n",
    "            \n",
    "        #check for optimal parameters\n",
    "        arima_model = auto_arima(proj_file_country, start_p=para_arima[0], d=para_arima[0], start_q=para_arima[0],\n",
    "                            max_p=para_arima[len(para_arima)-1], max_d=para_arima[len(para_arima)-1], max_q=para_arima[len(para_arima)-1],\n",
    "                            start_P=para_arima[0], D=para_arima[0], start_Q=para_arima[0],\n",
    "                            max_P=para_arima[len(para_arima)-1], max_D=para_arima[len(para_arima)-1], max_Q=para_arima[len(para_arima)-1],\n",
    "                            stepwise=False)\n",
    "\n",
    "        #split the data to train and test set\n",
    "        size = int(len(proj_file_country)*0.8)\n",
    "        X_train, X_test = proj_file_country[0:size], proj_file_country[size:len(proj_file_country)]\n",
    "\n",
    "        #train the model with data training set\n",
    "        if arima_model.get_params()['seasonal_order'][-1] > 1:\n",
    "            model = SARIMAX(X_train, \n",
    "                            order=arima_model.get_params()['order'], \n",
    "                            seasonal_order=arima_model.get_params()['seasonal_order'],\n",
    "                            initialization='approximate_diffuse')\n",
    "        else:\n",
    "            model = SARIMAX(X_train, \n",
    "                            order=arima_model.get_params()['order'],\n",
    "                            initialization='approximate_diffuse')\n",
    "\n",
    "        result = model.fit()\n",
    "        \n",
    "        \n",
    "        #number of years to predict\n",
    "        if freq_check==2:\n",
    "            year_pred = len(pd.date_range((proj_file_country.index[-1]+relativedelta(months=1)),\n",
    "                            pd.to_datetime('2024',format='%Y',exact=False),\n",
    "                            freq='MS'))-1\n",
    "                \n",
    "        elif freq_check==4:\n",
    "            year_pred = len(pd.date_range((proj_file_country.index[-1]+relativedelta(months=3)),\n",
    "                            pd.to_datetime('2024',format='%Y',exact=False),\n",
    "                            freq='QS'))-1\n",
    "\n",
    "        else:\n",
    "            year_pred = 2024-(proj_file_country.index[-1].year+1)\n",
    "        \n",
    "\n",
    "        #check the prediction of both training and testing set\n",
    "        train_pred = result.predict(0,size)\n",
    "        test_pred = result.predict(size, len(proj_file_country)+year_pred)\n",
    "        \n",
    "\n",
    "        #plot the prediction\n",
    "\n",
    "        ax[i][0].plot(train_pred, label= 'Training Set Prediction')\n",
    "        ax[i][0].plot(proj_file_country['Value'], label= 'Actual Figures')\n",
    "        ax[i][0].plot(test_pred, label= 'Testing Set Prediction')\n",
    "        ax[i][0].set_title('{} - Prediction with Splitting Dataset'.format(c))\n",
    "        ax[i][0].legend()\n",
    "\n",
    "\n",
    "        #train the model without splitting the data as seems like to more accurate for only predicting one year after\n",
    "        #approximate_diffuse to avoid a possible error called LU Decomposition, not sure if there is any side-effect\n",
    "        if arima_model.get_params()['seasonal_order'][-1] > 1:\n",
    "            model = SARIMAX(proj_file_country, \n",
    "                            order=arima_model.get_params()['order'], \n",
    "                            seasonal_order=arima_model.get_params()['seasonal_order'],\n",
    "                            initialization='approximate_diffuse')\n",
    "        else:\n",
    "            model = SARIMAX(proj_file_country, \n",
    "                            order=arima_model.get_params()['order'],\n",
    "                            initialization='approximate_diffuse')\n",
    "\n",
    "        result = model.fit()\n",
    "\n",
    "        #check the prediction of both training and testing set\n",
    "        train_pred = result.predict(0,size)\n",
    "        test_pred = result.predict(size, len(proj_file_country)+year_pred)\n",
    "\n",
    "        #plot the prediction\n",
    "\n",
    "        ax[i][1].plot(train_pred, label= 'Training Set Prediction')\n",
    "        ax[i][1].plot(proj_file_country['Value'], label= 'Actual Figures')\n",
    "        ax[i][1].plot(test_pred, label= 'Testing Set Prediction')\n",
    "        ax[i][1].set_title('{} - Prediction without Splitting Dataset'.format(c))\n",
    "        ax[i][1].legend()\n",
    "\n",
    "        #include the prediction value in the dataset\n",
    "        pred_df = pd.DataFrame(test_pred[-(year_pred+1):]).rename(columns={'predicted_mean':'Value'})\n",
    "        proj_file_country = pd.concat([proj_file_country,pred_df])\n",
    "        proj_file_country['Country Code'] = c\n",
    "\n",
    "        #concat all the individual back to one big table\n",
    "        proj_file_proj = pd.concat([proj_file_proj,proj_file_country])\n",
    "\n",
    "    plt.title(title_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return proj_file_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d588c",
   "metadata": {},
   "source": [
    "## Disaggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3764a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_disagg(country:[], agg_fil:[[]], col_name:str):\n",
    "    \n",
    "    final_proj_disagg = pd.DataFrame()\n",
    "    fig,ax = plt.subplots(2,4,figsize=(30,16))\n",
    "    \n",
    "    for i, c in enumerate(country):\n",
    "        \n",
    "        agg_fil_new = agg_fil.copy()\n",
    "        \n",
    "        if col_name in ts_proj:\n",
    "            agg_fil_new = agg_fil_new.reset_index()\n",
    "            agg_fil_new['TIME'] = pd.to_datetime(agg_fil_new['index'].astype(str)).dt.to_period('Y').dt.to_timestamp(how='end')\n",
    "            \n",
    "        else:\n",
    "            agg_fil_new['TIME'] = pd.to_datetime(agg_fil_new['TIME']+'-12-31',exact=False)\n",
    "            \n",
    "        agg_fil_new = agg_fil_new[agg_fil_new['Country Code']==c].set_index('TIME')[['Value']]\n",
    "        agg_fil_new['Value'] = agg_fil_new['Value'].astype(float)\n",
    "\n",
    "        oidx = agg_fil_new.index\n",
    "        nidx = pd.date_range(oidx.min(), oidx.max(), freq='M')\n",
    "        res = agg_fil_new.reindex(oidx.union(nidx)).interpolate('cubicspline')\n",
    "        \n",
    "        res['Country Code'] = c\n",
    "        final_proj_disagg = pd.concat([final_proj_disagg,res])\n",
    "        \n",
    "        ax.flatten()[i].plot(res['Value'], label= 'Disaggregated Level')\n",
    "        ax.flatten()[i].plot(agg_fil_new['Value'], label= 'Actual Level')\n",
    "        ax.flatten()[i].set_title('{} - {} Disaggregated Time Series'.format(c, col_name))\n",
    "        ax.flatten()[i].legend()\n",
    "        \n",
    "    plt.show()\n",
    "    display(final_proj_disagg)\n",
    "    print('-'*122)\n",
    "    \n",
    "    final_proj_disagg = final_proj_disagg.reset_index().rename(columns={'Value':col_name})\n",
    "    final_proj_disagg['Year'] = final_proj_disagg['index'].dt.year\n",
    "    final_proj_disagg['Month'] = final_proj_disagg['index'].dt.month\n",
    "        \n",
    "    return final_proj_disagg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85946b4a",
   "metadata": {},
   "source": [
    "### Read all and combine all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd70d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.path.join(cur_dir,\"OECD\")\n",
    "\n",
    "files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "PreAvoidDup = \"1\"\n",
    "Agg_OECD_df = Agg_df.copy()\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    if file_path.lower().endswith(\"csv\"):\n",
    "        try:\n",
    "            if 'actual' in file_path.lower():\n",
    "                content = pd.read_csv(file_path, quoting=3)\n",
    "                Agg_OECD_df = pd.merge(Agg_OECD_df, content.drop('index',axis=1), \n",
    "                                  on=[\"Country Code\", \"Year\", \"Month\"], how=\"left\")\n",
    "                \n",
    "            else:\n",
    "                content = pd.read_csv(file_path, quoting=3)\n",
    "                content = OECD_clean_input(content)\n",
    "                content[\"Value\"] = content[\"Value\"].astype(float)\n",
    "                content[\"Country Code\"] = content[\"LOCATION\"].map(OECDCountryCodeSub)\n",
    "                content[\"Year_Month\"] = content[\"TIME\"]\n",
    "                Agg_OECD_df = pd.merge(Agg_OECD_df, content[[\"Country Code\", \"Year_Month\", \"Value\"]], \n",
    "                                  on=[\"Country Code\", \"Year_Month\"], how=\"left\")\n",
    "\n",
    "                Agg_OECD_df.rename(columns={\"Value\":\"Value\"+PreAvoidDup}, inplace=True)\n",
    "\n",
    "                PreAvoidDup += \"1\"            \n",
    "                        \n",
    "        except pd.errors.ParserError as e:\n",
    "            print(f\"Error parsing {file}: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"Skipping non-csv file: {file}\")\n",
    "        \n",
    "OldColumnName = Agg_OECD_df.columns.tolist()\n",
    "NewColumnName = ['Year_Month', 'Year', 'Month', 'Quarter', 'Country', 'Country Code', 'FiscalYear', 'FiscalMonth', \n",
    "                 'FiscalYearMonth', 'Average Regulatory Index Score', 'Household Disposable Income per Capita', \n",
    "                 'Elderly Population Proportion', 'GDP (in millions)', 'Healthcare Spending Proportion',\n",
    "                 'Parmaceutical Spending Proportion', 'Population Level', 'Tertiary Education Level Proportion',\n",
    "                 'Inflation Rate', 'Interest Rate', 'Unemployment Rate']\n",
    "old_new_col_name_map = dict(zip(OldColumnName, NewColumnName))\n",
    "\n",
    "Agg_OECD_df.rename(columns=old_new_col_name_map, inplace=True)\n",
    "\n",
    "Agg_OECD_df[\"Elderly Population Proportion\"] = Agg_OECD_df.apply(lambda df: df[\"Elderly Population Proportion\"]*1000000 if df[\"Elderly Population Proportion\"]<0.001\n",
    "                                                      else df[\"Elderly Population Proportion\"], axis=1)\n",
    "\n",
    "Agg_OECD_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2f6059",
   "metadata": {},
   "source": [
    "## Financial Statement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf14138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Financial data function for reading IS and BS\n",
    "\n",
    "def fin_data_read(folder_path:str, data_needed:str, first_file_toread:str, skiprows:int, usecols:[], filename:str,) -> pd.DataFrame:\n",
    "\n",
    "    folder_path = folder_path\n",
    "\n",
    "    files = [f for f in os.listdir(folder_path)]\n",
    "\n",
    "    sheet_pattern = re.compile(r\"^(?:Company_)?[A-Za-z]{{2}}{}$\".format(data_needed))\n",
    "\n",
    "    fin_df = pd.DataFrame({\"YearMonth_Country_FinItems\":[\"Year_Month\",\"Country\"]})\n",
    "    sup_df = pd.read_excel(first_file_toread, \n",
    "                           sheet_name=\"Company{}\".format(data_needed), skiprows=skiprows, usecols=[0])\n",
    "    sup_df.columns = fin_df.columns\n",
    "    fin_df = pd.concat([fin_df, sup_df], ignore_index=True)\n",
    "\n",
    "    Avoid_Dup = \"1\"\n",
    "\n",
    "    for file in files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "\n",
    "        if file.upper().startswith(filename):\n",
    "            file_ext_name = os.path.basename(file_path)\n",
    "            file_year_month = file_ext_name[-9:-5]\n",
    "\n",
    "            file_df = pd.ExcelFile(file_path)\n",
    "            all_sheet_names = file_df.sheet_names\n",
    "\n",
    "            for sheet_name in all_sheet_names:\n",
    "                if sheet_name != \"Company_BS\":\n",
    "                    if sheet_pattern.match(sheet_name):\n",
    "                        file_country = sheet_name\n",
    "                        sup_data = file_df.parse(sheet_name, skiprows=skiprows, usecols=usecols)\n",
    "                        sup_data.columns = [\"YearMonth_Country_FinItems\",\"YearMonth_Country_FinData\"+Avoid_Dup]\n",
    "\n",
    "                        month_data = pd.DataFrame({\"YearMonth_Country_FinItems\":[\"Year_Month\",\"Country\"],\n",
    "                                                  \"YearMonth_Country_FinData\"+Avoid_Dup:[file_year_month,file_country]})\n",
    "\n",
    "                        fin1_df = pd.concat([month_data,sup_data], ignore_index=True)\n",
    "                        try:\n",
    "                            fin1_df.iloc[9,0] = fin1_df.iloc[9,0].replace(\"Payments\",\"Payments_sub\")\n",
    "\n",
    "                        except:\n",
    "                            pass\n",
    "                        fin_df = pd.merge(fin_df, fin1_df, on=\"YearMonth_Country_FinItems\", how=\"outer\")\n",
    "                        fin_df = fin_df.dropna(subset=[\"YearMonth_Country_FinItems\"], how=\"all\")\n",
    "\n",
    "                        Avoid_Dup += \"1\"\n",
    "\n",
    "    return fin_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05965b10",
   "metadata": {},
   "source": [
    "## Datahub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c3e35",
   "metadata": {},
   "source": [
    "#### Price Range Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_quantiles = pd.qcut(outbound_excl_cus[\"OrderPriceSales\"], q=[0,0.2,0.4,0.6,0.8,1.0], labels=[\"Very Low\",\"Low\", \"Medium\", \"High\", \"Very High\"])\n",
    "\n",
    "outbound_price_range = outbound_excl_cus.loc[:]\n",
    "outbound_price_range[\"Price Categorization\"] = price_quantiles\n",
    "\n",
    "outbound_price_range = outbound_price_range.groupby([\"Year_Month\",\"Country Code\",\"Price Categorization\"])[\"Deliver Export Turnover\"].sum().reset_index()\n",
    "outbound_tot_price = outbound_price_range.groupby([\"Year_Month\",\"Country Code\"])[\"Deliver Export Turnover\"].transform(\"sum\")\n",
    "\n",
    "outbound_price_range[\"Price Range Revenue Proportion\"] = outbound_price_range[\"Deliver Export Turnover\"]/outbound_tot_price\n",
    "\n",
    "outbound_price_range_pivot = outbound_price_range.drop(\"Deliver Export Turnover\", axis=1).pivot_table(index=[\"Year_Month\",\"Country Code\"],\n",
    "                                                                                                     columns =[\"Price Categorization\"],\n",
    "                                                                                                     values=[\"Price Range Revenue Proportion\"],\n",
    "                                                                                                     aggfunc=\"sum\",\n",
    "                                                                                                     fill_value=0)\n",
    "\n",
    "outbound_price_range_pivot.columns = [f\"{col[1]}_{col[0]}\" for col in outbound_price_range_pivot.columns]\n",
    "outbound_price_range_pivot.reset_index(inplace=True)\n",
    "\n",
    "outbound_price_range_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4102ce",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b117dc3",
   "metadata": {},
   "source": [
    "### All Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3861ee",
   "metadata": {},
   "source": [
    "#### PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3736648",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer = Pipeline(steps=[('NaNnum',SimpleImputer(strategy='mean')),\n",
    "                                  ('scale',StandardScaler())])\n",
    "\n",
    "bi_cat_transformer = Pipeline(steps=[('NaNcat',SimpleImputer(strategy='most_frequent')),\n",
    "                                  ('onehot',OneHotEncoder())])\n",
    "\n",
    "preprocessor_pca = ColumnTransformer(transformers=[('num_transformer',num_transformer,target_features+numeric_features),\n",
    "                                               ('bi_cat_transformer',bi_cat_transformer,binary_categorical_features)])\n",
    "\n",
    "pipeline_pca = Pipeline(steps=[('preprocessor',preprocessor_pca),\n",
    "                           ('pca',PCA(n_components=2))])\n",
    "\n",
    "pca = pipeline_pca.fit_transform(Agg_df_infadj[target_features+numeric_features+binary_categorical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette('Set2',len(country))\n",
    "country_color_dict = dict(zip(country,palette))\n",
    "country_color = [country_color_dict[x] for x in Agg_df_infadj[\"Country\"]]\n",
    "\n",
    "for sample in range(len(pca)):\n",
    "    plt.scatter(pca[sample][0],pca[sample][1],color=country_color[sample],s=8)\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('PCA Analysis')\n",
    "legend_handles = [plt.Line2D([0],[0], marker='o', color='w', label=label,\n",
    "                             markerfacecolor=palette[i], markersize=8) for i, label in enumerate(country)]\n",
    "plt.legend(handles=legend_handles, title='Countries',loc='center left', fontsize=8, bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173206f3",
   "metadata": {},
   "source": [
    "#### Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary statistics for numeric features\n",
    "\n",
    "summary_stat_num = Agg_df_infadj[target_features+numeric_features].describe()\n",
    "summary_stat_num.loc[\"+3_std\"] = summary_stat_num.loc[\"mean\"]+(summary_stat_num.loc[\"std\"]*3)\n",
    "summary_stat_num.loc[\"-3_std\"] = summary_stat_num.loc[\"mean\"]-(summary_stat_num.loc[\"std\"]*3)\n",
    "\n",
    "summary_stat_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aede980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Possible outliers outside of +/-3 std\n",
    "pos_out_num = Agg_df_infadj[(np.abs(stats.zscore(Agg_df_infadj[target_features+numeric_features]))>3).any(axis=1)]\n",
    "pos_out_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f5d27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Correlation Matrix\n",
    "corr_num = Agg_df_infadj[target_features+numeric_features].corr()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(corr_num, xticklabels=corr_num.columns, yticklabels=corr_num.columns, cmap=\"RdBu\")\n",
    "plt.show()\n",
    "\n",
    "display(corr_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211d272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(corr_num, alpha=1, figsize=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42b6c5",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd399932",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stat_bi_cat = Agg_df_infadj[binary_categorical_features].describe(include=[\"category\",\"object\"])\n",
    "\n",
    "summary_stat_bi_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda97c9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from github (dython)\n",
    "associations(Agg_df_infadj[binary_categorical_features], nom_nom_assoc=\"theil\", figsize=(15, 15), cmap='RdBu')[\"corr\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b33559",
   "metadata": {},
   "source": [
    "## Main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbee709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from github (dython)\n",
    "associations(Agg_df_infadj[target_features+main_numeric_features+main_binary_categorical_features], nom_num_assoc=\"correlation_ratio\", figsize=(15, 15), cmap='RdBu')['corr']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f1a71",
   "metadata": {},
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4a937",
   "metadata": {},
   "source": [
    "#### Numeric main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not transformed with log\n",
    "\n",
    "fig, axes = plt.subplots(6, 4, figsize=(25,20))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(target_features+main_numeric_features):\n",
    "        sns.histplot(x=Agg_df_infadj[target_features+main_numeric_features].iloc[:,i], ax=ax)\n",
    "        ax.axvline(Agg_df_infadj[target_features+main_numeric_features].iloc[:,i].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "        ax.axvline(Agg_df_infadj[target_features+main_numeric_features].iloc[:,i].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
    "        ax.set_title(Agg_df_infadj[target_features+main_numeric_features].columns[i])\n",
    "        ax.set_xlabel(xlabel='')\n",
    "        ax.set_ylabel(ylabel='')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "fig.suptitle(\"Univariate Analysis of Key Numeric Features\", fontsize='x-large')       \n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d498c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Transformed with log\n",
    "\n",
    "fig, axes = plt.subplots(6, 4, figsize=(25,20))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(target_features+main_numeric_features):\n",
    "        sns.histplot(x=np.log1p(Agg_df_infadj[target_features+main_numeric_features]).iloc[:,i], ax=ax)\n",
    "        ax.axvline(np.log1p(Agg_df_infadj[target_features+main_numeric_features]).iloc[:,i].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
    "        ax.axvline(np.log1p(Agg_df_infadj[target_features+main_numeric_features]).iloc[:,i].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
    "        ax.set_title(Agg_df_infadj[target_features+main_numeric_features].columns[i])\n",
    "        ax.set_xlabel(xlabel='')\n",
    "        ax.set_ylabel(ylabel='')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "fig.suptitle(\"Univariate Analysis of Key Numeric Features (Log Transformed)\", fontsize='x-large')       \n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d71b8d",
   "metadata": {},
   "source": [
    "#### Categorical main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810815f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(10,10), gridspec_kw={'hspace':0.6})\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < 2:\n",
    "        sns.countplot(x=Agg_df_infadj[main_binary_categorical_features].iloc[:,i], ax=ax, palette='Blues')\n",
    "        ax.set_title(Agg_df_infadj[main_binary_categorical_features].columns[i])\n",
    "        ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "        ax.set_xlabel(xlabel='')\n",
    "        ax.set_ylabel(ylabel='')\n",
    "    else:\n",
    "        sns.countplot(x=Agg_df_infadj[main_binary_categorical_features].iloc[:,i], ax=ax, palette='Blues')\n",
    "        ax.set_title(Agg_df_infadj[main_binary_categorical_features].columns[i])\n",
    "        ax.set_xlabel(xlabel='')\n",
    "        ax.set_ylabel(ylabel='')\n",
    "    \n",
    "fig.suptitle(\"Univariate Analysis of Key Binary and Categorical Features\",fontsize='x-large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d42b1f",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebab501",
   "metadata": {},
   "source": [
    "#### Numeric main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42868c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(Agg_df_infadj, vars=target_features+main_numeric_features, hue='Country', corner=True, palette='Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19a1bd8",
   "metadata": {},
   "source": [
    "#### Categorical main Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312506d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,4,figsize=(35,15),sharey='row', sharex='col')\n",
    "\n",
    "\n",
    "for t_i, target in enumerate(target_features):\n",
    "    for p_i, pred in enumerate(main_binary_categorical_features):\n",
    "        if t_i==0 and p_i==0:\n",
    "            ax=axes[t_i][p_i]\n",
    "            sns.boxplot(Agg_df_infadj,\n",
    "                x=pred, \n",
    "                y=target, \n",
    "                hue=\"Country\", \n",
    "                palette='Set2',\n",
    "                ax=ax)\n",
    "            ax.set_xlabel(xlabel='')\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "            ax.get_legend().remove()\n",
    "            \n",
    "        elif t_i==1 and p_i==0:\n",
    "            ax=axes[t_i][p_i]\n",
    "            sns.boxplot(Agg_df_infadj,\n",
    "                x=pred, \n",
    "                y=target, \n",
    "                hue=\"Country\", \n",
    "                palette='Set2',\n",
    "                ax=ax)\n",
    "            ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "            ax.get_legend().remove()\n",
    "            \n",
    "        elif t_i==1 and p_i in range(1,len(main_binary_categorical_features)):\n",
    "            ax=axes[t_i][p_i]\n",
    "            sns.boxplot(Agg_df_infadj,\n",
    "                x=pred, \n",
    "                y=target, \n",
    "                hue=\"Country\", \n",
    "                palette='Set2',\n",
    "                ax=ax)\n",
    "            if p_i == 1:\n",
    "                ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n",
    "            ax.set_ylabel(ylabel='')\n",
    "            ax.get_legend().remove()\n",
    "        \n",
    "        else:\n",
    "            ax=axes[t_i][p_i]\n",
    "            sns.boxplot(Agg_df_infadj,\n",
    "                x=pred, \n",
    "                y=target, \n",
    "                hue=\"Country\", \n",
    "                palette='Set2',\n",
    "                ax=ax)\n",
    "            ax.set_xlabel(xlabel='')\n",
    "            ax.set_ylabel(ylabel='')\n",
    "            ax.get_legend().remove()\n",
    "            \n",
    "fig.legend(handles,labels,loc='outside right center', title=\"Country\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78992616",
   "metadata": {},
   "source": [
    "## Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Train the model\n",
    "\n",
    "folds = 5\n",
    "cv = KFold(n_splits=folds-1)\n",
    "param_grid = {'rf__max_depth': [10,20,30,40,50],\n",
    "              'rf__n_estimators': np.arange(50,350,50)}\n",
    "\n",
    "preprocessor_rf = ColumnTransformer(transformers=[('num_transformer',num_transformer,numeric_features),\n",
    "                                               ('bi_cat_transformer',bi_cat_transformer,binary_categorical_features)])\n",
    "\n",
    "pipeline_rf = Pipeline(steps=[('preprocessor',preprocessor_rf),\n",
    "                           ('rf',RandomForestRegressor(n_estimators=100, random_state=0))])\n",
    "\n",
    "\n",
    "GSCV_rf = GridSearchCV(estimator=pipeline_rf, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=cv)\n",
    "\n",
    "X = Agg_df_infadj[numeric_features+binary_categorical_features]\n",
    "\n",
    "for i in target_features:\n",
    "    y = SimpleImputer(strategy='mean').fit_transform(Agg_df_infadj[[i]]).reshape(-1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/folds, random_state=0)\n",
    "\n",
    "    GSCV_rf.fit(X_train,y_train)\n",
    "\n",
    "    \n",
    "    ####Model Evaluation and Prediction\n",
    "\n",
    "    print('GridSearchCV Random Forest NMAE Best Score for Target Feature - {}: {:.3f}'.format(i,GSCV_rf.best_score_))\n",
    "    print('GridSearchCV Random Forest NMAE Best Score for Target Feature - {}: {}'.format(i,GSCV_rf.best_params_))\n",
    "    print('-'*100)\n",
    "\n",
    "    y_pred = GSCV_rf.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print('MSE - {}: {:.3f}'.format(i,mse))\n",
    "    print('RMSE - {}: {:.3f}'.format(i,rmse))\n",
    "    print('R2 - {}: {:.3f}'.format(i,r2))\n",
    "    print('-'*100)\n",
    "\n",
    "    plt.scatter(y_test, y_pred)\n",
    "    plt.xlabel('Actual Value')\n",
    "    plt.ylabel('Predicted Value')\n",
    "    plt.title('Random Forest Predictions (All Features) - {}'.format(i))\n",
    "    coeff = np.polyfit(y_test,y_pred,1)\n",
    "    p_line = np.poly1d(coeff)\n",
    "    plt.plot(y_test,p_line(y_test),color='darkorange')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    ####Feature Importance (MDI - Impurity)\n",
    "\n",
    "    rf_feat_name = [name.split('__')[1] for name in GSCV_rf.best_estimator_[:-1].get_feature_names_out()]\n",
    "    rf_feat_imp = GSCV_rf.best_estimator_[-1].feature_importances_\n",
    "\n",
    "    feat_imp_df = pd.Series(rf_feat_imp, index=rf_feat_name).sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.barplot(y=feat_imp_df.index,x=feat_imp_df,palette='Blues_r')\n",
    "    plt.title('Random Forest Feature Importance (MDI) - {}'.format(i),fontsize='xx-large')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    ####Feature Importance (MDA - Accurcy - Permutation)\n",
    "    \n",
    "    #On training data\n",
    "    feat_MDA_result = permutation_importance(\n",
    "        GSCV_rf, X_train, y_train, n_repeats=10, random_state=0, n_jobs=2)\n",
    "\n",
    "    sorted_importance_idx = feat_MDA_result.importances_mean.argsort()\n",
    "\n",
    "    feat_imp_pi_df = pd.DataFrame(feat_MDA_result.importances[sorted_importance_idx].T,\n",
    "                                  columns=X.columns)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.boxplot(feat_imp_pi_df,orient='h',palette='Blues')\n",
    "    plt.axvline(x=0, color=\"darkorange\", linestyle=\"--\", linewidth=2)\n",
    "    plt.title('Random Forest Feature Importance_Training Set (MDA) - {}'.format(i),fontsize='xx-large')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #On testing data\n",
    "    feat_MDA_result = permutation_importance(\n",
    "        GSCV_rf, X_test, y_test, n_repeats=10, random_state=0, n_jobs=2)\n",
    "\n",
    "    sorted_importance_idx = feat_MDA_result.importances_mean.argsort()\n",
    "\n",
    "    feat_imp_pi_df = pd.DataFrame(feat_MDA_result.importances[sorted_importance_idx].T,\n",
    "                                  columns=X.columns)\n",
    "\n",
    "    plt.figure(figsize=(20,20))\n",
    "    sns.boxplot(feat_imp_pi_df,orient='h',palette='Blues')\n",
    "    plt.axvline(x=0, color=\"darkorange\", linestyle=\"--\", linewidth=2)\n",
    "    plt.title('Random Forest Feature Importance_Test Set (MDA) - {}'.format(i),fontsize='xx-large')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169a8a0",
   "metadata": {},
   "source": [
    "# DAG Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e150dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DAG Basic Examples\n",
    "\n",
    "DAG_chain = gr.Digraph()\n",
    "DAG_chain.edge(\"X\", \"T\")\n",
    "DAG_chain.edge(\"T\", \"Y\")\n",
    "DAG_chain\n",
    "\n",
    "DAG_fork = gr.Digraph()\n",
    "DAG_fork.edge(\"X\", \"T\")\n",
    "DAG_fork.edge(\"X\", \"Y\")\n",
    "DAG_fork\n",
    "\n",
    "DAG_collider = gr.Digraph()\n",
    "DAG_collider.edge(\"X\", \"Y\")\n",
    "DAG_collider.edge(\"T\", \"Y\")\n",
    "DAG_collider\n",
    "\n",
    "DAG_chain.render(\"Chain_DAG\",view=True,format=\"png\")\n",
    "DAG_fork.render(\"Fork_DAG\",view=True,format=\"png\")\n",
    "DAG_collider.render(\"Collider_DAG\",view=True,format=\"png\")\n",
    "\n",
    "#Backdoor Adjustment Example\n",
    "\n",
    "DAG_b4Backdoor = gr.Digraph()\n",
    "DAG_b4Backdoor.edge(\"X\", \"T\")\n",
    "DAG_b4Backdoor.edge(\"X\", \"Y\")\n",
    "DAG_b4Backdoor.edge(\"T\", \"Y\")\n",
    "DAG_b4Backdoor\n",
    "\n",
    "DAG_afterBackdoor = gr.Digraph()\n",
    "DAG_afterBackdoor.edge(\"X\", \"Y\")\n",
    "DAG_afterBackdoor.edge(\"T\", \"Y\")\n",
    "DAG_afterBackdoor\n",
    "\n",
    "DAG_b4Backdoor.render(\"b4Backdoor_DAG\",view=True,format=\"png\")\n",
    "DAG_afterBackdoor.render(\"afterBackdoor_DAG\",view=True,format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a2144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the macro, industry and micro features after the selection above for building official final DAG for the model\n",
    "\n",
    "macro_features_adj = [col for col in macro_features if col in numeric_features + binary_categorical_features]\n",
    "industry_features_adj = [col for col in industry_features if col in numeric_features + binary_categorical_features]\n",
    "micro_features_adj = [col for col in micro_features if col in numeric_features + binary_categorical_features]\n",
    "\n",
    "#df for final DAG\n",
    "df_graph = df_EDA[target_feature + country_indicator + time_indicator + macro_features_adj + \n",
    "                  industry_features_adj + micro_features_adj]\n",
    "df_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0957b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#edges for final DAG for the model\n",
    "\n",
    "edges_to_EDITDA = [(c, 'EBITDA_Thousands') for c in df_graph.iloc[:, 1: len(df_graph.columns)].columns]\n",
    "\n",
    "edges_from_coun_time_to_macro = [(c, o)\n",
    "                                 for c in df_graph.iloc[:, 1: df_graph.columns.get_loc('GDP_Millions')].columns\n",
    "                                 for o in df_graph.iloc[:, df_graph.columns.get_loc('GDP_Millions'): df_graph.columns.get_loc('RegulatoryIndex')].columns]\n",
    "    \n",
    "edges_from_macro = [(c, o) \n",
    "                    for c in df_graph.iloc[:, 1: df_graph.columns.get_loc('RegulatoryIndex')].columns\n",
    "                    for o in df_graph.iloc[:, df_graph.columns.get_loc('RegulatoryIndex'): len(df_graph.columns)].columns]\n",
    "\n",
    "edges_from_Reg = [('RegulatoryIndex', o) \n",
    "                  for o in df_graph.iloc[:, df_graph.columns.get_loc('RegulatoryIndex') + 1: len(df_graph.columns)].columns]\n",
    "\n",
    "edges_from_Pharm = [('PharmacyDensity_Per100k', o) \n",
    "                  for o in df_graph.iloc[:, df_graph.columns.get_loc('PharmacyDensity_Per100k') + 1: len(df_graph.columns)].columns]\n",
    "\n",
    "edges_to_MarketShare_p1 = [(c, 'Market_EUR') \n",
    "                           for c in df_graph.iloc[:, df_graph.columns.get_loc('RegulatoryIndex'): df_graph.columns.get_loc('Market_EUR')].columns]\n",
    "edges_to_MarketShare_p2 = [(c, 'Market_EUR') \n",
    "                           for c in df_graph.iloc[:, df_graph.columns.get_loc('Market_EUR') + 1: len(df_graph.columns)].columns]\n",
    "\n",
    "other_edges = [('GDP_Millions', 'HealthcareSpendProp'),\n",
    "               ('GDP_Millions', 'UnemploymentRate'),\n",
    "               ('PopulationLevel_Millions', 'HealthcareSpendProp'),\n",
    "               ('PopulationLevel_Millions', 'UnemploymentRate'),\n",
    "               ('NumWholesaleWarehouses', 'StockDays'),\n",
    "               ('NumDistinctCustomersServed', 'StockDays'),\n",
    "               ('OutboundServiceLevel', 'StockDays'),\n",
    "               ('NumDistinctCustomersServed', 'DebtorDays'),\n",
    "               ('NumDistinctSuppliersDealtWith', 'StockDays'),\n",
    "               ('InboundServiceLevel', 'StockDays'),\n",
    "               ('NumDistinctSuppliersDealtWith', 'CreditorDays')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfbd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_graph = nx.DiGraph(\n",
    "    edges_to_EDITDA +\n",
    "    edges_from_coun_time_to_macro +\n",
    "    edges_from_macro +\n",
    "    edges_from_Reg + \n",
    "    edges_from_Pharm +\n",
    "    edges_to_MarketShare_p1 + \n",
    "    edges_to_MarketShare_p2 +\n",
    "    other_edges\n",
    ")\n",
    "\n",
    "nx.draw(causal_graph, \n",
    "        with_labels=True,\n",
    "        node_size=2000,\n",
    "        font_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the DAG to dowhy and identify backdoor path for each treatment\n",
    "\n",
    "backdoor_path = {}\n",
    "\n",
    "for col in main_numeric_features + main_binary_categorical_features:\n",
    "\n",
    "    model = CausalModel(\n",
    "       data=df_graph,\n",
    "       treatment=col,\n",
    "       outcome='EBITDA_Thousands',\n",
    "       graph=\"\\n\".join(nx.generate_gml(causal_graph))\n",
    "    )\n",
    "\n",
    "    identified_estimand = model.identify_effect(\n",
    "        proceed_when_unidentifiable=True,\n",
    "        method_name='maximal-adjustment'\n",
    "    )\n",
    "\n",
    "    print('Backdoor length: ', len(identified_estimand.get_backdoor_variables()))\n",
    "    print('Backdoor variables: ', identified_estimand.get_backdoor_variables())\n",
    "    print(identified_estimand)\n",
    "    print('-'*100)\n",
    "\n",
    "    backdoor_path[col] = identified_estimand.get_backdoor_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bce653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excluded variables in backdoor path\n",
    "\n",
    "for col in main_numeric_features + main_binary_categorical_features:\n",
    "    path = backdoor_path[col]\n",
    "    notback = dict()\n",
    "    notback[col] = [c for c in country_indicator+time_indicator+numeric_features+binary_categorical_features if c not in path+[col]]\n",
    "    print(notback)\n",
    "    print(len(notback[col]))\n",
    "    \n",
    "notback "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726cbce",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e841c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just dummy and clustering for OLS \n",
    "\n",
    "df_linear_dummy = pd.get_dummies(df_EDA[country_indicator+time_indicator+binary_categorical_features], \n",
    "                                 drop_first=True, dtype=float).drop('Country_Sweden', axis=1)\n",
    "df_linear_num = df_EDA[target_feature+numeric_features]\n",
    "\n",
    "df_linear = pd.concat([df_linear_num, df_linear_dummy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(df_linear['EBITDA_Thousands'], \n",
    "               sm.add_constant(df_linear.loc[:, df_linear.columns != 'EBITDA_Thousands'])).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cbe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_result_df = pd.DataFrame()\n",
    "\n",
    "ols_result_df = pd.concat([ols_result_df, model.params], axis=1).rename(columns={0: 'Coefficient'})\n",
    "ols_result_df = pd.concat([ols_result_df, model.bse], axis=1).rename(columns={0: 'Std. Error'})\n",
    "ols_result_df = pd.concat([ols_result_df, model.pvalues], axis=1).rename(columns={0: 'P-value'})\n",
    "ols_result_df = pd.concat([ols_result_df, model.conf_int()], axis=1).rename(columns={0: 'CI Lower Bound (2.5%)', 1: 'CI Upper Bound (97.5%)'})\n",
    "\n",
    "ols_result_df.to_excel('ols_result.xlsx')\n",
    "\n",
    "ols_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bedff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_result_df = pd.read_excel('ols_result.xlsx')\n",
    "\n",
    "for col in ols_result_df.columns:\n",
    "    if col != 'Unnamed: 0':\n",
    "        if col != 'P-value':\n",
    "            ols_result_df[col] = ols_result_df[col].round(2)\n",
    "\n",
    "        else:\n",
    "            ols_result_df[col] = ols_result_df[col].round(3)\n",
    "        \n",
    "ols_result_df['Method'] = 'OLS'\n",
    "        \n",
    "ols_result_df = ols_result_df.rename(columns={'Unnamed: 0':'Feature'})\n",
    "ols_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6578e60",
   "metadata": {},
   "source": [
    "## Causal Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db324f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the data with standard scaler, poly and one-hot encoding (poly only happens to Lasso)\n",
    "\n",
    "def sc_poly_ohe_preprocess(df, ml_type, column):\n",
    "    \n",
    "    #treatment variable specific backdoor path\n",
    "    backdoor = backdoor_path[column]  \n",
    "    #num col adjusted for both treatment and backdoor variables \n",
    "    num_col_no_treat = [col for col in numeric_features if col in backdoor]\n",
    "    #cat col adjusted for both treatment and backdoor variables \n",
    "    cat_col_no_treat = [col for col in binary_categorical_features if col in backdoor]\n",
    "    #country col adjusted for both treatment and backdoor variables\n",
    "    adj_country_indicator = [col for col in country_indicator if col in backdoor]\n",
    "    #time col adjusted for both treatment and backdoor variables\n",
    "    adj_time_indicator = [col for col in time_indicator if col in backdoor]\n",
    "    \n",
    "    #StandardScaler transform\n",
    "    sc = StandardScaler().set_output(transform='pandas')\n",
    "    df_sc = sc.fit_transform(df[num_col_no_treat])\n",
    "\n",
    "    if ml_type == 'Lasso':\n",
    "        #poly no interaction transform\n",
    "        df_poly_num2 = np.power(df_sc, 2).rename(columns=lambda x: x+'_2nd')\n",
    "        df_poly_num2 = df_poly_num2 - df_poly_num2.mean(axis=0)\n",
    "        df_poly_num3 = np.power(df_sc, 3).rename(columns=lambda x: x+'_3rd')\n",
    "        df_poly_num3 = df_poly_num3 - df_poly_num3.mean(axis=0)\n",
    "        df_poly_all = pd.concat([df_sc, df_poly_num2, df_poly_num3], axis=1)\n",
    "    \n",
    "        #dummy transform\n",
    "        df_dummy = pd.get_dummies(df[adj_country_indicator+adj_time_indicator+cat_col_no_treat], dtype=float).drop('Country_Sweden', axis=1)\n",
    "        df_poly_dummy = df_poly_all.join(df_dummy)\n",
    "        \n",
    "        #create interaction terms\n",
    "        poly = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n",
    "        df_poly_dummy_interact = poly.fit_transform(df_poly_dummy)\n",
    "        poly_cols = poly.get_feature_names_out(df_poly_dummy.columns)\n",
    "        df_poly_dummy_interact = pd.DataFrame(df_poly_dummy_interact, columns=poly_cols)\n",
    "        \n",
    "        filtered_binary = df_poly_dummy_interact.columns[df_poly_dummy_interact.nunique()<=2]\n",
    "\n",
    "        #All the duplicated categorical variables interaction\n",
    "        duplicate_interact_same_var = [col for col in filtered_binary if col.split('_')[0] in col.split('_')[1]]\n",
    "\n",
    "        ##Drop the duplicated interacting variables (e.g. RegulatoryIndex_LowImpact X RegulatoryIndex_HighImpact)\n",
    "        #All the categorical variables and categorical variables with interaction\n",
    "        df_poly_dummy_interact = df_poly_dummy_interact.drop(duplicate_interact_same_var, axis=1)\n",
    "\n",
    "        df_final = pd.concat([df[target_feature+[column]], df_poly_dummy_interact], axis=1)\n",
    "\n",
    "    else:\n",
    "        #dummy transform\n",
    "        df_dummy = pd.get_dummies(df[adj_country_indicator+adj_time_indicator+cat_col_no_treat], dtype=float).drop('Country_Sweden', axis=1)\n",
    "        \n",
    "        df_final = pd.concat([df[target_feature+[column]], df_sc, df_dummy], axis=1)\n",
    "    \n",
    "    #Shuffle the dataset\n",
    "    df_final = df_final.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for running all the treatment in one go in different CML models \n",
    "\n",
    "def cml_implement(df, ml_type, cml_type, sensitivity=False):\n",
    "    \n",
    "    cml_final_result = pd.DataFrame()\n",
    "    \n",
    "    if cml_type == 'dml':\n",
    "        cml_final_score = {'ml_y': {}, 'ml_t': {}}\n",
    "        \n",
    "    else:\n",
    "        cml_final_score = {'yhat_that': {}, 'ml_y': {}, 'ml_t': {}}\n",
    "    \n",
    "    for feature in main_numeric_features + main_binary_categorical_features:      \n",
    "\n",
    "        t_2_start = perf_counter()\n",
    "        \n",
    "        if cml_type == 'dml':\n",
    "            if feature in main_numeric_features:\n",
    "                result_all, score_dict, _ = dml_wrapper(df, ml_type, 'num', feature)\n",
    "\n",
    "            else:\n",
    "                result_all, score_dict, _ = dml_wrapper(df, ml_type, 'cat', feature)     \n",
    "        \n",
    "        else:\n",
    "            if feature in main_numeric_features:\n",
    "                result_all, score_dict, _ = cf_wrapper(df, ml_type, 'num', feature)\n",
    "\n",
    "            else:\n",
    "                result_all, score_dict, _ = cf_wrapper(df, ml_type, 'cat', feature)    \n",
    "\n",
    "        t_2_stop = perf_counter()\n",
    "\n",
    "        print(result_all)\n",
    "        print(score_dict)\n",
    "        print('Time used this round: {:.4f} seconds'.format(t_2_stop - t_2_start))\n",
    "        print('-'*100)  \n",
    "\n",
    "        cml_final_result = pd.concat([cml_final_result, result_all])\n",
    "        \n",
    "        if cml_type != 'dml':\n",
    "            cml_final_score['yhat_that'].update(score_dict['yhat_that'])\n",
    "            \n",
    "        cml_final_score['ml_y'].update(score_dict['ml_y'])\n",
    "        cml_final_score['ml_t'].update(score_dict['ml_t'])\n",
    "    \n",
    "    return (cml_final_result, cml_final_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffdf0ff",
   "metadata": {},
   "source": [
    "### DML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produce dml output for a single feature\n",
    "\n",
    "def dml_wrapper(df, ml_type, data_type, column):\n",
    "    \n",
    "    #penalty parameter for tuning logistic\n",
    "    Cs = 0.0001*np.logspace(0, 4, 10)\n",
    "    \n",
    "    #parameters for tuning RF\n",
    "    max_features = ['sqrt', 'log2']\n",
    "    max_depth = [3, 5, 7, 9]\n",
    "    min_samples_split = [10, 15, 20]\n",
    "    min_samples_leaf = [5, 10, 15]\n",
    "    n_estimators = np.arange(500,1000,100)\n",
    "    \n",
    "    par_grids_rf = {'ml_l': {'max_depth': max_depth,\n",
    "                             'max_features': max_features,\n",
    "                             'min_samples_split': min_samples_split,\n",
    "                             'min_samples_leaf': min_samples_leaf,\n",
    "                             'n_estimators': n_estimators},\n",
    "                    'ml_m': {'max_depth': max_depth,\n",
    "                             'max_features': max_features,\n",
    "                             'min_samples_split': min_samples_split,\n",
    "                             'min_samples_leaf': min_samples_leaf,\n",
    "                             'n_estimators': n_estimators}}\n",
    "    \n",
    "    df_processed = sc_poly_ohe_preprocess(df, ml_type, column)\n",
    "    \n",
    "    nuisance_score_dict = {'ml_y': {}, 'ml_t': {}}\n",
    "\n",
    "    if data_type == 'num':\n",
    "        dml_data = DoubleMLData(df_processed,\n",
    "                                y_col = target_feature[0],\n",
    "                                d_cols = column,\n",
    "                                x_cols = df_processed.iloc[:, 2:].columns.tolist())\n",
    "        \n",
    "        np.random.seed(22)\n",
    "        \n",
    "        if ml_type == 'Lasso':\n",
    "            ml_l = clone(LassoCV(max_iter=10000, random_state=22))\n",
    "            ml_m = clone(LassoCV(max_iter=10000, random_state=22))\n",
    "\n",
    "            obj_dml_plr = DoubleMLPLR(dml_data, ml_l=ml_l, ml_m=ml_m)\n",
    "\n",
    "        else: \n",
    "            ml_l = clone(RandomForestRegressor(random_state=22))\n",
    "            ml_m = clone(RandomForestRegressor(random_state=22))\n",
    "\n",
    "            obj_dml_plr = DoubleMLPLR(dml_data, ml_l=ml_l, ml_m=ml_m)\n",
    "            obj_dml_plr.tune(par_grids_rf, search_mode='grid_search', n_jobs_cv=-1)\n",
    "\n",
    "        obj_dml_plr.fit(n_jobs_cv=-1)\n",
    "        nuisance_score_dict['ml_y'][column] = obj_dml_plr.evaluate_learners()['ml_l'].item()\n",
    "        nuisance_score_dict['ml_t'][column] = obj_dml_plr.evaluate_learners()['ml_m'].item()\n",
    "        result_all = obj_dml_plr.summary              \n",
    "\n",
    "    else:\n",
    "        result_all = pd.DataFrame()\n",
    "\n",
    "        #to get the dummies of the targeted column and run the model of that one by one (except the first one that is dropped)\n",
    "        bi_cat_col = pd.DataFrame(df_processed.pop(column))\n",
    "        bi_cat_dum = pd.get_dummies(bi_cat_col[[column]], drop_first=True, dtype=float)\n",
    "        df_processed = pd.concat([bi_cat_dum, df_processed], axis=1)\n",
    "\n",
    "        #to calculate for x_cols\n",
    "        bin_cat_dum_num = len(bi_cat_dum.columns)\n",
    "\n",
    "        for column in bi_cat_dum.columns:\n",
    "            dml_data = DoubleMLData(df_processed,\n",
    "                                    y_col = target_feature[0],\n",
    "                                    d_cols = column,\n",
    "                                    x_cols = df_processed.iloc[:, 1+bin_cat_dum_num:].columns.tolist()) \n",
    "            \n",
    "            np.random.seed(22)\n",
    "            \n",
    "            if ml_type == 'Lasso':\n",
    "                ml_l = LassoCV(max_iter=10000, random_state=22)\n",
    "                ml_m = LogisticRegressionCV(penalty='l1', solver='liblinear', Cs=Cs, max_iter=10000, random_state=22)\n",
    "\n",
    "                obj_dml_plr = DoubleMLPLR(dml_data, ml_l=ml_l, ml_m=ml_m)\n",
    "\n",
    "            else:\n",
    "                ml_l = RandomForestRegressor(random_state=22)\n",
    "                ml_m = RandomForestClassifier(random_state=22)\n",
    "\n",
    "                obj_dml_plr = DoubleMLPLR(dml_data, ml_l=ml_l, ml_m=ml_m)\n",
    "                obj_dml_plr.tune(par_grids_rf, search_mode='grid_search', n_jobs_cv=-1)\n",
    "\n",
    "            obj_dml_plr.fit(n_jobs_cv=-1)\n",
    "            nuisance_score_dict['ml_y'][column] = obj_dml_plr.evaluate_learners()['ml_l'].reshape(-1).tolist()\n",
    "            nuisance_score_dict['ml_t'][column] = obj_dml_plr.evaluate_learners()['ml_m'].reshape(-1).tolist()\n",
    "            result = obj_dml_plr.summary\n",
    "\n",
    "            result_all = pd.concat([result_all, result])\n",
    "\n",
    "    return (result_all, nuisance_score_dict, obj_dml_plr)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86e63a",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all, _, _ = dml_wrapper(df_EDA, 'Lasso', \"cat\", \"RegulatoryIndex\")\n",
    "result_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a04f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_dml_final_result, lasso_dml_nuisance_final_score = cml_implement(df_EDA, 'Lasso', 'dml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_dml_final_result.to_excel('lasso_dml_final_result.xlsx')\n",
    "\n",
    "lasso_dml_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a823ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the results with proper titles\n",
    "\n",
    "lasso_dml_final_result_adj = pd.read_excel('lasso_dml_final_result.xlsx')\n",
    "\n",
    "lasso_dml_final_result_adj = lasso_dml_final_result_adj.rename(columns={'Unnamed: 0':'Feature', 'coef':'Coefficient',\n",
    "                                                                        'std err':'Std. Error', 'P>|t|': 'P-value',\n",
    "                                                                        '2.5 %':'CI Lower Bound (2.5%)', '97.5 %':'CI Upper Bound (97.5%)'}).drop('t', axis=1)\n",
    "\n",
    "for col in lasso_dml_final_result_adj.columns:\n",
    "    if col != 'Feature':\n",
    "        if col != 'P-value':\n",
    "            lasso_dml_final_result_adj[col] = lasso_dml_final_result_adj[col].round(2)\n",
    "\n",
    "        else:\n",
    "            lasso_dml_final_result_adj[col] = lasso_dml_final_result_adj[col].round(3)\n",
    "        \n",
    "lasso_dml_final_result_adj['Method'] = 'DML - Lasso'\n",
    "\n",
    "lasso_dml_final_result_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fac525f",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abd6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dml_final_result, rf_dml_nuisance_final_score = cml_implement(df_EDA, 'RandomForest', 'dml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dml_final_result.to_excel('rf_dml_final_result.xlsx')\n",
    "\n",
    "rf_dml_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the results with proper titles\n",
    "\n",
    "rf_dml_final_result_adj = pd.read_excel('rf_dml_final_result.xlsx')\n",
    "\n",
    "rf_dml_final_result_adj = rf_dml_final_result_adj.rename(columns={'Unnamed: 0':'Feature', 'coef':'Coefficient',\n",
    "                                                                  'std err':'Std. Error', 'P>|t|': 'P-value',\n",
    "                                                                  '2.5 %':'CI Lower Bound (2.5%)', '97.5 %':'CI Upper Bound (97.5%)'}).drop('t', axis=1)\n",
    "\n",
    "for col in rf_dml_final_result_adj.columns:\n",
    "    if col != 'Feature':\n",
    "        if col != 'P-value':\n",
    "            rf_dml_final_result_adj[col] = rf_dml_final_result_adj[col].round(2)\n",
    "\n",
    "        else:\n",
    "            rf_dml_final_result_adj[col] = rf_dml_final_result_adj[col].round(3)\n",
    "        \n",
    "rf_dml_final_result_adj['Method'] = 'DML - RF'\n",
    "\n",
    "rf_dml_final_result_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c0b3bd",
   "metadata": {},
   "source": [
    "### Causal Forest - Local Centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For transforming the final results of econml file with proper titles\n",
    "\n",
    "def econml_file(excel, method):\n",
    "    final_result_adj = pd.read_excel(excel)\n",
    "\n",
    "    for col in [17, 18]:\n",
    "        final_result_adj.loc[col, 'mean_point'] = final_result_adj.loc[col,'point_estimate']\n",
    "        final_result_adj.loc[col, 'stderr_mean'] = final_result_adj.loc[col, 'stderr']\n",
    "        final_result_adj.loc[col, 'ci_mean_lower'] = final_result_adj.loc[col, 'ci_lower']\n",
    "        final_result_adj.loc[col, 'ci_mean_upper'] = final_result_adj.loc[col, 'ci_upper']\n",
    "\n",
    "    final_result_adj.iloc[17, 0] = 'RegulatoryIndex_Low Impact'\n",
    "    final_result_adj.iloc[18, 0] = 'NumWholesaleWarehouses_None/Few'    \n",
    "\n",
    "    final_result_adj = final_result_adj.rename(columns={'feature':'Feature', \n",
    "                                                        'mean_point':'Coefficient',\n",
    "                                                        'stderr_mean':'Std. Error',\n",
    "                                                        'pvalue': 'P-value',\n",
    "                                                        'ci_mean_lower':'CI Lower Bound (2.5%)', \n",
    "                                                        'ci_mean_upper':'CI Upper Bound (97.5%)'})\n",
    "\n",
    "    final_result_adj = final_result_adj.drop(final_result_adj.iloc[:,7:].columns.tolist()+['zstat'], axis=1)\n",
    "\n",
    "    for col in final_result_adj.columns:\n",
    "        if col != 'Feature':\n",
    "            if col != 'P-value':\n",
    "                final_result_adj[col] = final_result_adj[col].round(2)\n",
    "\n",
    "            else:\n",
    "                final_result_adj[col] = final_result_adj[col].round(3)\n",
    "\n",
    "    final_result_adj['Method'] = method\n",
    "\n",
    "    return final_result_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a89f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product cf output for a single feature\n",
    "\n",
    "def cf_wrapper(df, ml_type, data_type, column, sensitivity=False):\n",
    "    \n",
    "    #parameters for tuning (lasso & logistic)\n",
    "    Cs = 0.0001*np.logspace(0, 4, 10)\n",
    "    \n",
    "    #parameters for tuning (random forest)\n",
    "    max_features = ['sqrt', 'log2']\n",
    "    max_depth = [3, 5, 7, 9]\n",
    "    min_samples_split = [10, 15, 20]\n",
    "    min_samples_leaf = [5, 10, 15]\n",
    "    n_estimators = np.arange(500,1000,100)\n",
    "    \n",
    "    df_processed = sc_poly_ohe_preprocess(df, ml_type, column)\n",
    "    \n",
    "    result_all = pd.DataFrame()\n",
    "    score_dict = {'yhat_that': {}, 'ml_y': {}, 'ml_t': {}}\n",
    "    \n",
    "    #Get the required columns for CF\n",
    "    Y = df_processed[target_feature1].values.ravel()\n",
    "    T = df_processed[column].values.ravel()\n",
    "    X = df_processed.iloc[:, 2:].values\n",
    "    \n",
    "    if ml_type == 'Lasso':\n",
    "        if data_type == 'num':\n",
    "            est_nonparam = CausalForestDML(model_y=LassoCV(max_iter=10000, random_state=22),\n",
    "                                           model_t=LassoCV(max_iter=10000, random_state=22), \n",
    "                                           cv=5, random_state=22)\n",
    "        \n",
    "        else:\n",
    "            est_nonparam = CausalForestDML(model_y=LassoCV(max_iter=10000, random_state=22),\n",
    "                                           model_t=LogisticRegressionCV(penalty='l1', solver='liblinear', \n",
    "                                                                        Cs=Cs, max_iter=10000, random_state=22), \n",
    "                                           discrete_treatment=True, cv=5, random_state=22)\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        first_stage_reg = lambda: GridSearchCV(estimator=RandomForestRegressor(random_state=22),\n",
    "                                                      param_grid={\n",
    "                                                          'max_depth': max_depth,\n",
    "                                                          'max_features': max_features,\n",
    "                                                          'min_samples_split': min_samples_split,\n",
    "                                                          'min_samples_leaf': min_samples_leaf,\n",
    "                                                          'n_estimators': n_estimators\n",
    "                                                      }, n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "                                                     )\n",
    "\n",
    "        first_stage_class = lambda: GridSearchCV(estimator=RandomForestClassifier(random_state=22),\n",
    "                                                      param_grid={\n",
    "                                                          'max_depth': max_depth,\n",
    "                                                          'max_features': max_features,\n",
    "                                                          'min_samples_split': min_samples_split,\n",
    "                                                          'min_samples_leaf': min_samples_leaf,\n",
    "                                                          'n_estimators': n_estimators\n",
    "                                                      }, n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "                                                     )\n",
    "        \n",
    "        if data_type == 'num':\n",
    "            model_y = clone(first_stage_reg().fit(X, Y).best_estimator_)\n",
    "            model_t = clone(first_stage_reg().fit(X, T).best_estimator_)\n",
    "\n",
    "            est_nonparam = CausalForestDML(model_y=model_y, model_t=model_t, n_estimators=1000, cv=5, random_state=22)\n",
    "            \n",
    "        else:\n",
    "            est_nonparam = CausalForestDML(model_y=first_stage_reg(), model_t=first_stage_class(), \n",
    "                                           discrete_treatment=True, n_estimators=1000, cv=5, random_state=22)\n",
    "    \n",
    "    np.random.seed(22)\n",
    "    est_nonparam.tune(Y, T, X=X, W=None)\n",
    "       \n",
    "    if sensitivity == True:\n",
    "        est_nonparam = est_nonparam.dowhy.fit(Y=Y, T=T, X=X, W=None,\n",
    "                                              outcome_names=target_feature1, \n",
    "                                              treatment_names=[column],\n",
    "                                              feature_names=df_processed.iloc[:, 2:].columns.tolist()\n",
    "                                             )\n",
    "    \n",
    "    else:\n",
    "        est_nonparam.fit(Y, T, X=X, W=None)\n",
    "       \n",
    "    if data_type == 'num':\n",
    "        result = est_nonparam.ate_inference(X, T0=0, T1=1).summary().tables[0].as_html()\n",
    "    \n",
    "    else:\n",
    "        result = est_nonparam.summary().tables[0].as_html()\n",
    "        \n",
    "    result_df = pd.read_html(result, header=0)[0]\n",
    "    result_df['feature'] = column\n",
    "    result_df = result_df.set_index('feature')\n",
    "    result_all = pd.concat([result_all, result_df])\n",
    "    \n",
    "    score_dict['yhat_that'][column] = est_nonparam.score_\n",
    "    score_dict['ml_y'][column] = est_nonparam.nuisance_scores_y\n",
    "    score_dict['ml_t'][column] = est_nonparam.nuisance_scores_t\n",
    "    \n",
    "    return (result_all, score_dict, est_nonparam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8459b19",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3602ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cf_final_result, lasso_cf_nuisance_final_score = cml_implement(df_EDA, 'Lasso', 'cf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e39c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cf_final_result.to_excel('lasso_cf_final_result.xlsx')\n",
    "\n",
    "lasso_cf_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0420c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cf_final_result_adj = econml_file('lasso_cf_final_result.xlsx', 'CF - Lasso')\n",
    "\n",
    "lasso_cf_final_result_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac6d34",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce30118",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cf_final_result, rf_cf_nuisance_final_score = cml_implement(df_EDA, 'RandomForest', 'cf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1365b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cf_final_result.to_excel('rf_cf_final_result.xlsx')\n",
    "\n",
    "rf_cf_final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cf_final_result_adj = econml_file('rf_cf_final_result.xlsx', 'CF - RF')\n",
    "\n",
    "rf_cf_final_result_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89bdcda",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d383efc",
   "metadata": {},
   "source": [
    "#### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e1fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the results with pvalue less than 0.05\n",
    "\n",
    "cml_all_results = pd.concat([ols_result_df,\n",
    "                             lasso_dml_final_result_adj, rf_dml_final_result_adj,\n",
    "                             lasso_cf_final_result_adj, rf_cf_final_result_adj\n",
    "                            ]) \n",
    "\n",
    "sig_features = set(cml_all_results[(cml_all_results['P-value'] < 0.05) &\n",
    "                                   (cml_all_results['Feature'].isin(main_numeric_features + main_binary_categorical_features+\n",
    "                                                                    ['RegulatoryIndex_Low Impact','NumWholesaleWarehouses_None/Few']))]['Feature'])\n",
    "methods = cml_all_results['Method'].unique()\n",
    "\n",
    "cml_all_results[(cml_all_results['P-value'] < 0.05) &\n",
    "                (cml_all_results['Feature'].isin(main_numeric_features + main_binary_categorical_features+\n",
    "                                                 ['RegulatoryIndex_Low Impact','NumWholesaleWarehouses_None/Few']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375709fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the confidence interval for drafting error bar format\n",
    "\n",
    "cml_all_results_err = cml_all_results.copy()\n",
    "\n",
    "cml_all_results_err['CI Lower Bound (2.5%)'] = cml_all_results_err['Coefficient'] - cml_all_results_err['CI Lower Bound (2.5%)']\n",
    "cml_all_results_err['CI Upper Bound (97.5%)'] = cml_all_results_err['CI Upper Bound (97.5%)'] - cml_all_results_err['Coefficient']\n",
    "\n",
    "cml_all_results_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for drawing graph with all the significant treatments\n",
    "\n",
    "def treatment_CI_comp_all(treatment, title, savename):\n",
    "    palette = sns.color_palette('Set2',len(treatment))\n",
    "    feature_color_dict = dict(zip(treatment, palette))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(35, 35))\n",
    "\n",
    "    for i, m in enumerate(methods):\n",
    "        for j, f in enumerate(treatment):\n",
    "\n",
    "            row = cml_all_results_err[(cml_all_results_err['Method'] == m) & (cml_all_results_err['Feature'] == f)]\n",
    "            x = i + j * 0.055  # Slight offset for better visibility\n",
    "            y = row['Coefficient'].values[0]\n",
    "            yerr = np.array([[row['CI Lower Bound (2.5%)'].values[0]], [row['CI Upper Bound (97.5%)'].values[0]]])\n",
    "\n",
    "            ax.errorbar(x, y, yerr=yerr, fmt=\"o\", markersize=20, capsize=10, capthick=8,\n",
    "                        elinewidth=8, color=feature_color_dict[f], label=f)\n",
    "\n",
    "    xticks_centers = np.arange(len(methods)) + (len(treatment) - 1) * 0.055 / 2\n",
    "    ax.set_xticks(xticks_centers)    \n",
    "    ax.set_xticklabels(methods)\n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('ATE with CI')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), loc='upper right', prop={'size':23}, title = 'Treatment')\n",
    "\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=3.5, alpha=0.5)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 30})\n",
    "    \n",
    "    plt.savefig(savename, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot only the features that are significant in at least 2 methods (OLS sig features excluded)\n",
    "\n",
    "sig_features_count = Counter(cml_all_results_err[(cml_all_results_err['P-value'] < 0.05) & \n",
    "                                                 (cml_all_results_err['Method'] != 'OLS')]['Feature'])\n",
    "\n",
    "sig_features_atleast2 = list({k: v for k, v in sig_features_count.items() if v >= 2}.keys())\n",
    "\n",
    "treatment_CI_comp_all(sig_features_atleast2, \n",
    "                     'Error Bars of Each Method for Each Significant Treatment\\n (At least two Occurrences in all Methods - OLS Excluded)',\n",
    "                     'SigTreatErrorAtleast2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd58ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_CI_comp_3(treatment, title, methods, savename):\n",
    "    palette = sns.color_palette('Set2',len(treatment))\n",
    "    feature_color_dict = dict(zip(treatment, palette))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 20))\n",
    "\n",
    "    for i, m in enumerate(methods):\n",
    "        for j, f in enumerate(treatment):\n",
    "\n",
    "            row = cml_all_results_err[(cml_all_results_err['Method'] == m) & (cml_all_results_err['Feature'] == f)]\n",
    "            x = i + j * 0.055 # Slight offset for better visibility\n",
    "            y = row['Coefficient'].values[0]\n",
    "            yerr = np.array([[row['CI Lower Bound (2.5%)'].values[0]], [row['CI Upper Bound (97.5%)'].values[0]]])\n",
    "\n",
    "            ax.errorbar(x, y, yerr=yerr, fmt=\"o\", markersize=15, capsize=8, capthick=5,\n",
    "                        elinewidth=5, color=feature_color_dict[f], label=f)\n",
    "\n",
    "    xticks_centers = np.arange(len(methods)) + (len(treatment) - 1) * 0.055 / 2\n",
    "    ax.set_xticks(xticks_centers)    \n",
    "    ax.set_xticklabels(methods)\n",
    "    ax.set_xlabel('Method')\n",
    "    ax.set_ylabel('ATE with CI')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    ax.legend(by_label.values(), by_label.keys(), loc='upper right', prop={'size':15}, title = 'Treatment')\n",
    "\n",
    "    plt.axhline(y=0, color='black', linestyle='--', linewidth=2.5, alpha=0.5)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    \n",
    "    plt.savefig(savename, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a510b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot only the features that are significant in at least 2 methods (OLS excluded from graph)\n",
    "\n",
    "methods_noOLS = cml_all_results[cml_all_results['Method'] != 'OLS']['Method'].unique()\n",
    "\n",
    "treatment_CI_comp_3(sig_features_atleast2, \n",
    "                    'Error Bars of Each Method for Each Significant Treatment - OLS excluded\\n (At least two Occurrences in all Methods)',\n",
    "                    methods_noOLS, \"SigTreatErrorAtleast2_noOLS.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ab014e",
   "metadata": {},
   "source": [
    "#### CF - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526e5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function for building graph for all the coefficient sizes for CF - Lasso\n",
    "\n",
    "def treatment_effect(df, title, palette, savename, legend=False):\n",
    "    fig, ax = plt.subplots(figsize = (20, 20))\n",
    "    sns.barplot(y=df['Coefficient'], x=df['Feature'], palette=palette)\n",
    "\n",
    "    colors = [bar.get_facecolor() for bar in ax.patches]\n",
    "    yerr = np.array([df['CI Lower Bound (2.5%)'].values, df['CI Upper Bound (97.5%)'].values])\n",
    "    for i in range(len(df)):\n",
    "        ax.errorbar(i, df['Coefficient'].values[i],\n",
    "                    yerr=[[df['CI Lower Bound (2.5%)'].values[i]], \n",
    "                          [df['CI Upper Bound (97.5%)'].values[i]]],\n",
    "                    fmt='none', color=colors[i], capsize=4, capthick=2, elinewidth=2)\n",
    "\n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=2.5, alpha=0.5)\n",
    "\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.set_xlabel('Treatment')\n",
    "    ax.set_ylabel('ATE with CI')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    if legend == True:\n",
    "        legend_elements = [plt.Line2D([0], [0], color=palette[0], lw=10, label='Significant'),\n",
    "                           plt.Line2D([0], [0], color=palette[-1], lw=10, label='Not Significant')]\n",
    "\n",
    "        plt.legend(handles=legend_elements, loc = 'upper right', fontsize=20)\n",
    "\n",
    "    plt.savefig(savename, bbox_inches='tight')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3644e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show treatment effect by pvalue for CF - Lasso\n",
    "\n",
    "cf_rf_err_pvalue = cml_all_results_err[cml_all_results_err['Method'] == 'CF - Lasso'].sort_values(by='P-value').reset_index(drop=True)\n",
    "\n",
    "feature_color_dict = {}\n",
    "\n",
    "num_sig_features = len(cf_rf_err_pvalue[cf_rf_err_pvalue['P-value'] < 0.05])\n",
    "\n",
    "palette = sns.color_palette('Set2', num_sig_features)\n",
    "for i in range(len(cf_rf_err_pvalue)):\n",
    "    if i < num_sig_features:\n",
    "        feature_color_dict[cf_rf_err_pvalue['Feature'][i]] = palette[0]\n",
    "    else:\n",
    "        feature_color_dict[cf_rf_err_pvalue['Feature'][i]] = palette[1]\n",
    "\n",
    "treatment_effect(cf_rf_err_pvalue, 'Treatment Effect by P-value', \n",
    "                 [feature_color_dict[f] for f in cf_rf_err_pvalue['Feature']],  'cflasso_pvalue.png', legend=True)\n",
    "plt.axvline(x=len(cf_rf_err_pvalue[cf_rf_err_pvalue['P-value'] < 0.05])-0.5,\n",
    "            color='black', linestyle='--', linewidth=2.5, alpha=0.5)\n",
    "\n",
    "plt.savefig('cflasso_pvalue.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fa77e1",
   "metadata": {},
   "source": [
    "### Robustness Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b35268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For fitting the CF - Lasso model again but with also dowhy environment for robustness check\n",
    "\n",
    "def cat_sensitivity_prep(df, ml_type, column):\n",
    "    \n",
    "    df_processed = sc_poly_ohe_preprocess(df, ml_type, column)\n",
    "    \n",
    "    if column in main_binary_categorical_features:\n",
    "        df_processed[column] = df_processed[column].cat.codes.astype('bool')\n",
    "\n",
    "    model = CausalModel(\n",
    "        data=df_processed,\n",
    "        treatment=column,\n",
    "        outcome=target_feature[0],\n",
    "        common_causes=df_processed.iloc[:, 2:].columns.tolist(),\n",
    "        effect_modifiers=df_processed.iloc[:, 2:].columns.tolist()\n",
    "    )\n",
    "\n",
    "    identified_estimand = model.identify_effect(\n",
    "        proceed_when_unidentifiable=True,\n",
    "        method_name='maximal-adjustment')\n",
    "    \n",
    "    print(identified_estimand)\n",
    "    \n",
    "    #parameters for tuning logistic regression\n",
    "    Cs = 0.0001*np.logspace(0, 4, 10)\n",
    "    \n",
    "    \n",
    "    np.random.seed(22)    \n",
    "    if column in main_binary_categorical_features:        \n",
    "        cf_lasso_estimate_dw = model.estimate_effect(identified_estimand,\n",
    "                                                        method_name=\"backdoor.econml.dml.CausalForestDML\",\n",
    "                                                        control_value=0,\n",
    "                                                        treatment_value=1,\n",
    "                                                        target_units=\"ate\",\n",
    "                                                        confidence_intervals=True,  \n",
    "                                                        method_params={\n",
    "                                                            'init_params': {'model_y': LassoCV(max_iter=10000, random_state=22),\n",
    "                                                                            'model_t': LogisticRegressionCV(penalty='l1', solver='liblinear', \n",
    "                                                                                       Cs=Cs, max_iter=10000, random_state=22), \n",
    "                                                                            \"discrete_treatment\":True},\n",
    "                                                            'fit_params': {}\n",
    "                                                        }\n",
    "                                                       )\n",
    "    \n",
    "    else:\n",
    "        cf_lasso_estimate_dw = model.estimate_effect(identified_estimand,\n",
    "                                                        method_name=\"backdoor.econml.dml.CausalForestDML\",\n",
    "                                                        control_value=0,\n",
    "                                                        treatment_value=1,\n",
    "                                                        target_units=\"ate\",\n",
    "                                                        confidence_intervals=True,  \n",
    "                                                        method_params={\n",
    "                                                            'init_params': {'model_y': LassoCV(max_iter=10000, random_state=22),\n",
    "                                                                            'model_t': LassoCV(max_iter=10000, random_state=22)},\n",
    "                                                            'fit_params': {}\n",
    "                                                        }\n",
    "                                                       )\n",
    "\n",
    "    print(cf_lasso_estimate_dw)\n",
    "    \n",
    "    return (cf_lasso_estimate_dw, model, identified_estimand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RegulatoryIndex dowhy & CF - Lasso\n",
    "\n",
    "cf_lasso_dw_reg, dag_reg, estimand_reg = cat_sensitivity_prep(df_EDA, 'Lasso', 'RegulatoryIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb19444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warehouse dowhy & CF - Lasso\n",
    "\n",
    "cf_lasso_dw_ware, dag_ware, estimand_ware = cat_sensitivity_prep(df_EDA, 'Lasso', 'NumWholesaleWarehouses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9b20c",
   "metadata": {},
   "source": [
    "#### Random Common Cause (CF - Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49115b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regulatory Index\n",
    "res_random_reg = dag_reg.refute_estimate(estimand_reg, cf_lasso_dw_reg, \n",
    "                                       method_name=\"random_common_cause\",\n",
    "                                       show_progress_bar=True, random_state=22, n_jobs=-1)\n",
    "\n",
    "print(res_random_reg)\n",
    "print('-'*100)\n",
    "\n",
    "#Warehouse\n",
    "res_random_ware = dag_ware.refute_estimate(estimand_ware, cf_lasso_dw_ware, \n",
    "                                        method_name=\"random_common_cause\",\n",
    "                                        show_progress_bar=True, random_state=22, n_jobs=-1)\n",
    "\n",
    "print(res_random_ware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7c487",
   "metadata": {},
   "source": [
    "#### Placebo Treatment (CF - Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regulatory Index\n",
    "res_placebo_reg = dag_reg.refute_estimate(estimand_reg, cf_lasso_dw_reg, \n",
    "                                             method_name=\"placebo_treatment_refuter\",  \n",
    "                                             placebo_type=\"permute\",\n",
    "                                             show_progress_bar=True, random_state=22, n_jobs=-1)\n",
    "\n",
    "print(res_placebo_reg)\n",
    "print('-'*100)\n",
    "\n",
    "#Warehouse\n",
    "res_placebo_ware = dag_ware.refute_estimate(estimand_ware, cf_lasso_dw_ware, \n",
    "                                             method_name=\"placebo_treatment_refuter\",  \n",
    "                                             placebo_type=\"permute\",\n",
    "                                             show_progress_bar=True, random_state=22, n_jobs=-1)\n",
    "\n",
    "print(res_placebo_ware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e086a66",
   "metadata": {},
   "source": [
    "#### Data Subset Validation (CF - Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ed103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regulatory Index\n",
    "res_subset_reg = dag_reg.refute_estimate(estimand_reg, cf_lasso_dw_reg, \n",
    "                                             method_name=\"data_subset_refuter\",  \n",
    "                                             subset_fraction=0.8,\n",
    "                                             show_progress_bar=True, random_state=22, n_jobs=-1)\n",
    "\n",
    "print(res_subset_reg)\n",
    "print('-'*100)\n",
    "\n",
    "#Warehouse\n",
    "res_subset_ware = dag_ware.refute_estimate(estimand_ware, cf_lasso_dw_ware, \n",
    "                                             method_name=\"data_subset_refuter\",  \n",
    "                                             subset_fraction=0.8,\n",
    "                                             show_progress_bar=True, random_state=22, n_jobs=-1)\n",
    "\n",
    "print(res_subset_ware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac4a332",
   "metadata": {},
   "source": [
    "#### Sensitivity Analysis (DML - Lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d9352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For building the sensitivity analysis table\n",
    "\n",
    "def dml_sensitivity(df, ml_type, data_type, column):\n",
    "    \n",
    "    _, _, model = dml_wrapper(df, ml_type, data_type, column)\n",
    "    \n",
    "    ss_df = pd.DataFrame()\n",
    "    \n",
    "    model.sensitivity_analysis()\n",
    "    print(model.sensitivity_summary)    \n",
    "\n",
    "    ss_df['ci_lower'] = model.sensitivity_params['ci']['lower']\n",
    "    ss_df['theta_lower'] = model.sensitivity_params['theta']['lower']\n",
    "    ss_df['theta'] = model.coef\n",
    "    ss_df['theta_upper'] = model.sensitivity_params['theta']['upper']\n",
    "    ss_df['ci_upper'] = model.sensitivity_params['ci']['upper']\n",
    "    ss_df['rv'] = model.sensitivity_params['rv']\n",
    "    ss_df['rva'] = model.sensitivity_params['rva']\n",
    "    ss_df['feature'] = column\n",
    "    ss_df = ss_df.set_index('feature')\n",
    "    \n",
    "    model.sensitivity_plot()\n",
    "\n",
    "    return (ss_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DML - Lasso regulatory index with sensitivity analysis\n",
    "\n",
    "ss_df, dml_model = dml_sensitivity(df_EDA, 'Lasso', 'cat', 'RegulatoryIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b549a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_model.sensitivity_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DML - Lasso number of warehouses with sensitivity analysis\n",
    "\n",
    "ss_df_ware, dml_model_ware = dml_sensitivity(df_EDA, 'Lasso', 'cat', 'NumWholesaleWarehouses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe2e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_model_ware.sensitivity_plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
